---
layout: mathpage
title: 感知器学习 
date: 2015-07-22
categories: 机器学习 自然语言处理
tags: 感知器
onewords: 感知器是神经网络与支持向量机的基础，引入了超平面的概念，使用了错误驱动+SGD的更新方式。是简单好用的分类方法。是一种判别模型。
---
> 感知器在数据集划分超平面做分类，模型参数包含权值向量 $$ w $$ 和偏移 $$ b $$  ;模型的损失函数定义了错分点到超平面的距离和；模型的更新机制是基于`错误驱动`的`SGD`方法。最后，感知器是一种线性分类方法，是判别模型。

##基本思想

我们把数据表示为向量空间中的点（使用一个向量表示）。分类问题就是要将数据划分到相应类别，对应到向量空间，就是要划分数据点——如果是二元分类，我们就说将点划分为正例(POSITIVE)与负例(NEGATIVE)；如果是多元分类，我们输出一个标签(LABEL)。

感知器是在向量空间中构建一个或多个超平面，对数据点进行划分。如果是二元分类，只需要一个超平面即可。如果是多元分类 ,设类别数为N($$ N > 2 $$)，则需要N个超平面。

> 感知器的超平面思想，更一般的可以称为`线性分类面`、`线性决策边界`等。这种思想是SVM（支持向量机），NN（神经网络）的基础。

##超平面定义

在向量空间（欧式空间）$$ \mathbb{R}^n $$中，定义 平面法向量$$ W \in \mathbb{R}^n $$ , 定义平面偏移 $$ b \in \mathbb{R}^1 $$ , 定义空间中的点（实例）$$ X \in \mathbb{R}^n $$ ， 则超平面可表示为： 

$$ y = W * X + b$$

>   欧式空间： 又称为欧几里得空间（Euclidean Space）。用$$ \mathbb{R} $$表示实数域，对任意一个正整数n，实数的n元组的全体构成了$$ \mathbb{R} $$上的一个n维向量空间，用$$ \mathbb{R}^n $$表示，有时称之为**实数坐标空间**(来自[维基百科](https://zh.wikipedia.org/wiki/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%A9%BA%E9%97%B4)).*通俗的来说，初高中学习的二维坐标空间、三维立体空间都是欧式空间。*

###关于平面的一些强调

1. $$ y = W * X + b $$中，**W就是平面的法向量**。

2. 任意点到平面的距离：设点为$$ x \in \mathbb{R}^n $$ , 则点到平面$$ y = W * X + b $$的距离为 $$ L = \frac {\vert W*x + b \vert}{\Vert W \Vert_2}$$ （*即把x带入平面方程中求出的值取绝对值，除以平面法向量W的模（二范数）*） 。这也被称为**几何间隔** 。

##模型

###什么是模型

在监督学习过程中，模型就是要学习的概率分布或者决策函数。感知器所属的判别模型，通常是确定一个决策函数，来将输入映射到输出。

###感知器模型

####决策函数：

对于二元分类，
    
$$ y = sign(W*X + b) $$
    
W是权值向量，即平面法向量 $$ W \in \mathbb{R}^n $$，b是偏移 , $$ b \in \mathbb{R} $$；
X是输入向量，$$ X \in \mathbb{R}^n $$ , y是输出label，$$ y \in \{-1,+1\} $$

对于多元分类,

$$ y_i = \arg_{i \in I} \max(W_i * X + b_i) $$

设输出类别数为$$ n $$ , 下标集合$$ I = \{ 0 , 1 , ... , n-1\} , i \in I $$ ，输出标签 $$ y_i \in \{y_0 , y_1 , ... , y_{n-1}\} $$ . 前面提到，n个类别，需要n个超平面。则对应的权值向量集合为$$ \{ W_0 , W_1 , ... , W_{n-1} \} $$ ，对应的b为$$ b \in \{ b_1 , b_2 , ... , b_{n-1} \}$$。

####直观定义

二元分类，把点带入平面，结果为正即为正例，否则为负例。

多元分类，取点到所有超平面中距离（严格来说，指带入平面方程的值，有正负）最大的超平面对应的标签作为输出。

##优化目标




