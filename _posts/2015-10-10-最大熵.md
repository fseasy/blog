---
layout: mathpage
title: 最大熵模型学习
date: 2015-10-10
categories: 机器学习
tags: 最大熵就是满足约束条件下最随机、最无偏、最平均的猜测
onewords: 本文主要根据资料介绍了最大熵原理，最大熵模型的定义、约束条件、学习方法（解的求解）等。待补充。
---
> 本文首先介绍最大熵的原理及含义；接着在分类问题下介绍了最大熵模型的定义，约束条件以及学习（求解）的方法。[待补充]。

###最大熵(Maximum Entropy)原理<sup> <a href="#ref1">[1]</a> <a href="#ref2">[2]</a> <a href="#ref3">[3]</a> </sup>

####熵

最原始的熵，是从热力学中引入的。

$$\Delta S = \frac{Q}{T} $$

$\Delta S$表示Entropy , 即熵；而Q表示能量，T表示温度。

一位中国学者在翻译“Entropy”时，由于考虑“Entropy是能量与温度T的商，且与火有些关系”，就把“Entropy”翻译为了“熵”这个高大上的名字。

熵表示一个系统在不受外界外界干扰时其内部最稳定的状态。任何粒子的常态都是随机运动，也就是无序运动。如果要让粒子呈现有序化，必须耗费能量。所以，温度（热量）可以看做“有序化”的度量，而熵则可以看作无序化的度量。<sup>[1]</sup>

在信息论中，熵特指`信息熵`。后面出现的熵均指信息熵。

信息熵的计算公式为： 

$$ H(x) = \sum_x { p(x)\log(\frac{1}{p(x)})} $$ 

其中X是一个随机变量，$p(x)$表示$X = x$时的概率。熵的取值范围为$0 \le H(x) \le \log(\left \| X \right \|)$。$\left \| X \right \|$指随机变量$X$的可能取值的数量。

关于信息熵的含义，有很多种解释，比如说“熵是接收的每条消息中包含的信息的平均量“<sup><a href="#ref4">[4]</a></sup>.我们把$\frac{1}{p(x)}$称作自信息量，这个还是比较有意义的。一般来说，在固定长度的信息中，出现概率越少的东西，表达的信息往往是越多的。所以把概率取倒数、再去对数来表示自信息量——如果某个字符出现概率为1，那么这个字符什么信息也没有。信息量乘以概率，再求和，比较明显的求期望（平均值）公式。所以能够表示信息的平均量。

其他说法如：”信息熵定义了信息的不确定程度“，”定义了信息的混乱度“。这个倒过来想也是有道理的：我们刚刚的例子，当信息全为一个字符时，熵最小，为0。此时信息的不确定程度、混乱度是最低的。这个比较直观。当我们的X服从均匀分布，即每种可能值都是等可能的，可想而知这种情况下信息也是最混乱的，最不确定的（如果我们拿到一段信息，去猜其可能的值，当各可能值是等可能的，那么猜测最不确定；如果知道哪个值的概率最大，那么是更加确定的），此时从熵公式的数学定义上也可以说明，此时熵也是最大的($\log(\left \| X \right \|)$)。由此我们得出一个结论：*”熵的大小，正比于信息的不确定程度（信息的混乱度）“*。

强调一遍，**”熵越大，代表信息越不确定，越混乱“**。

####最大熵原理

有了前面的铺垫，最大熵原理就比较好说了：*最大熵原理，就是在满足约束条件下，对未知分布最随机、最不确定的推断。*

定义包含两点：

1. 要满足约束

2. 最随机，最不确定的推断

关于”最随机，最不确定“的理解：

前面说过，最不确定的情况，就是随机变量满足均匀分布的情况。所以，**最大熵也是求一个在约束条件下最均匀的分布。**这说明最大熵在预测未知分布时，是未做任何有偏假设的。

###最大熵模型

最大熵模型，就是在分类问题下，用最大熵原理求一个满足约束条件的分类模型，并将该模型最为分类最优的模型。

分类问题下，我们的模型定义为刻画类别$Y$在随机变量$X$存在条件下的分布——输入一个实例$x$（抽象描述为向量$\vec x$），给出其在各类别的概率——给出后验概率$P(Y \| X)$ 。

> ”后验概率“一说，为个人观点，未在参考资料上看到。

####目标

目标当然是满足约束条件下熵最大的模型。约束条件在下一小节讲。这里我们首先确定，我们要度量什么熵呢。

前面讲到模型就是给出后验概率，所以当然我们刻画的熵就是这个后验概率的熵，即**条件熵**$H(Y|X)。当保证这该熵最大时，就保证了该模型在约束条件下对$X$的分布有最均匀的估计，即满足最大熵原理。

条件熵的公式为：
    
$$H(Y|X) = H(X,Y) - H(X) = - \sum_{x,y}{p(x,y)\log{p(y|x)}}$$

>关于条件熵公式的推导，更多详细定义，可以查看[1](#ref1) , 或者搜索维基百科。这里只直接记住结论，暂时对该公式过程不清楚。

在样本集中，我们用经验分布$\tilde p(x)$近似表示$X$原始分布$p(x)$ ， 故我们的条件熵公式可以写为：

$$H(Y|X) = - sum_{x,y}{\tilde p(x) p(y | x) \log(p(y | x))}$$

其中$\tilde p(x) = \frac{count(x)}{count(all)}$，在样本集给定条件下为已知。

经过这样的变化，条件熵仅与我们的模型目标$p(y \| x)$有关。

我们的模型目标定义为

$$ p(y | x) = \arg_{p(y|x)} \max H(Y|X) = \arg\_{p(y|x)} \max { - \sum\_{x,y}{\tilde p(x) p(y|x) \log(p(y|x))} }$$

####约束条件


<span id="ref1">[1]</span> [最大熵模型中的数学推导](http://blog.csdn.net/v_july_v/article/details/40508465)

<span id="ref2">[2]</span> [条件随机场综述](http://www.paper.edu.cn/download/downPaper/201001-479)

<span id="ref3">[3]</span> 李航 · 《统计学习方法》

<span id="ref4">[4]</span> [维基百科](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))





