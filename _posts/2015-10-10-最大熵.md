---
layout: mathpage
title: 最大熵模型学习
date: 2015-10-10
categories: 机器学习
tags: 最大熵就是满足约束条件下最随机、最无偏、最平均的猜测
onewords: 本文主要根据资料介绍了最大熵原理，最大熵模型的定义、约束条件、学习方法（解的求解）等。待补充。
---
> 本文首先介绍最大熵的原理及含义；接着在分类问题下介绍了最大熵模型的定义，约束条件以及学习（求解）的方法。[待补充]。

###最大熵(Maximum Entropy)原理<sup> <a href="#ref1">[1]</a> <a href="#ref2">[2]</a> <a href="#ref3">[3]</a> </sup>

####熵

最原始的熵，是从热力学中引入的。

$$\Delta S = \frac{Q}{T} $$

$\Delta S$表示Entropy , 即熵；而Q表示能量，T表示温度。

一位中国学者在翻译“Entropy”时，由于考虑“Entropy是能量与温度T的商，且与火有些关系”，就把“Entropy”翻译为了“熵”这个高大上的名字。

熵表示一个系统在不受外界外界干扰时其内部最稳定的状态。任何粒子的常态都是随机运动，也就是无序运动。如果要让粒子呈现有序化，必须耗费能量。所以，温度（热量）可以看做“有序化”的度量，而熵则可以看作无序化的度量。<sup>[1]</sup>

在信息论中，熵特指`信息熵`。后面出现的熵均指信息熵。

信息熵的计算公式为： 

$$ H(x) = \sum_x { p(x)\log(\frac{1}{p(x)})} $$ 

其中X是一个随机变量，$p(x)$表示$X = x$时的概率。熵的取值范围为$0 \le H(x) \le \log(\left \| X \right \|)$。$\left \| X \right \|$指随机变量$X$的可能取值的数量。

关于信息熵的含义，有很多种解释，比如说“熵是接收的每条消息中包含的信息的平均量“<sup><a href="#ref4">[4]</a></sup>.我们把$\frac{1}{p(x)}$称作自信息量，这个还是比较有意义的。一般来说，在固定长度的信息中，出现概率越少的东西，表达的信息往往是越多的。所以把概率取倒数、再去对数来表示自信息量——如果某个字符出现概率为1，那么这个字符什么信息也没有。信息量乘以概率，再求和，比较明显的求期望（平均值）公式。所以能够表示信息的平均量。

其他说法如：”信息熵定义了信息的不确定程度“，”定义了信息的混乱度“。这个倒过来想也是有道理的：我们刚刚的例子，当信息全为一个字符时，熵最小，为0。此时信息的不确定程度、混乱度是最低的。这个比较直观。当我们的X服从均匀分布，即每种可能值都是等可能的，可想而知这种情况下信息也是最混乱的，最不确定的（如果我们拿到一段信息，去猜其可能的值，当各可能值是等可能的，那么猜测最不确定；如果知道哪个值的概率最大，那么是更加确定的），此时从熵公式的数学定义上也可以说明，此时熵也是最大的($\log(\left \| X \right \|)$)。由此我们得出一个结论：*”熵的大小，正比于信息的不确定程度（信息的混乱度）“*。

强调一遍，**”熵越大，代表信息越不确定，越混乱“**。

####最大熵原理

有了前面的铺垫，最大熵原理就比较好说了：*最大熵原理，就是在满足约束条件下，对未知分布最随机、最不确定的推断。*

定义包含两点：

1. 要满足约束

2. 最随机，最不确定的推断

关于”最随机，最不确定“的理解：

前面说过，最不确定的情况，就是随机变量满足均匀分布的情况。所以，**最大熵也是求一个在约束条件下最均匀的分布。**这说明最大熵在预测未知分布时，是未做任何有偏假设的。

###最大熵模型

最大熵模型，就是在分类问题下，用最大熵原理求一个满足约束条件的分类模型，并将该模型作为分类最优的模型。

分类问题下，我们的模型定义为刻画类别$Y$在随机变量$X$存在条件下的分布——输入一个实例$x$（抽象描述为向量$\vec x$），给出其在各类别的概率——给出后验概率$P(Y \| X)$ 。

> ”后验概率“一说，为个人观点，未在参考资料上看到。

####目标

目标当然是满足约束条件下熵最大的模型。约束条件在下一小节讲。这里我们首先确定，我们要度量什么熵呢。

前面讲到模型就是给出后验概率，所以当然我们刻画的熵就是这个后验概率的熵，即**条件熵**$H(Y \| X)$。当保证这该熵最大时，就保证了该模型在约束条件下对$X$的分布有最均匀的估计，即满足最大熵原理。

条件熵的公式为：
    
$$H(Y|X) = H(X,Y) - H(X) = - \sum_{x,y}{p(x,y)\log{p(y|x)}}$$

>关于条件熵公式的推导，更多详细定义，可以查看[1](#ref1) , 或者搜索维基百科。这里只直接记住结论，暂时对该公式过程不清楚。

在样本集中，我们用经验分布$\tilde p(x)$近似表示$X$原始分布$p(x)$ ， 故我们的条件熵公式可以写为：

$$H(Y|X) = - sum_{x,y}{\tilde p(x) p(y | x) \log(p(y | x))}$$

其中$\tilde p(x) = \frac{count(x)}{count(all)}$，在样本集给定条件下为已知。

经过这样的变化，条件熵仅与我们的模型目标$p(y \| x)$有关。

我们的模型目标定义为

$$ p(y | x) = \arg_{p(y|x)} \max H(Y|X) = \arg_{p(y|x)} \max { - \sum_{x,y}{\tilde p(x) p(y|x) \log(p(y|x))} }$$

####约束条件

**引入特征函数**

我们的模型，是要去表述随机变量X与Y的关系，可以用$P(X , Y)$来描述。不过，如果我们从实例这个角度去描述，显然数据是很稀疏的，很难在有限数据集上去良好的刻画随机变量X与Y的分布。在这样的情况下，我们引入**特征函数**这一个概念。特征函数，在概率论上与分布函数是存在密切关系的（*看了一些，不过依然不清楚，待以后可能再补充*）。特征函数可以完整的刻画随机变量的分布情况，且比分布函数具有更好的性质。当然具体到实际的机器学习问题上，特征函数就是从某个方面刻画了实例与对应类别的关系。

比如分词，对实例（“我”，“S”），一个特征函数可能是：

$$f_{cur\_gram}(x,y) = \begin{cases}
1 , &x =“我” 且 y=“S”\\
0 , &其他情况
\end{cases}$$

另一个特征函数是：

$$f_{cur\_type}(x,y) = \begin{cases}
1 , &type(x) = "普通字符" 且 y = “S” \\
0 , &其他情况 
\end{cases}$$

针对一个实例，从不同方面抽取特征，来刻画随机变量的内在属性。这可以认为是特征函数的作用。

回到我们的目标，要去表述随机变量X、Y间的关系，我们选择用特征函数$f(X,Y)$来刻画。

更进一步地，**用特征函数的期望来刻画**。

**特征函数上如何约束**

我们从样本集的角度以及待求取的模型的角度分别刻画随机变量X、Y。即是，*要求 从样本集求得的特征函数期望与从模型角度求得的特征函数期望相等。*

我们将从样本集角度求得的特征函数期望称为特征函数的*经验期望*，记为$E\_{\tilde p}(f(x,y))$；将模型角度求得的期望称为特征函数的*模型期望*，记为$E\_{p}(f(x,y))$。

当模型足够好时，二者应该相等 ，故在特征函数上的约束可以表示为：

$$\begin{equation} \label{expequal}
E_{\tilde p}(f_i(x,y)) = E_{p}(f_i(x,y)) , i = 1 , 2 , \cdots ,m 
\end{equation}$$

其中m为特征函数数量。

再说这两个期望该如何求。

期望的公式可以表示为$E(X) = \sum\_x{ x p(x)}$ ,其中$p(x)$是$X=x$时的概率。由此我们的特征函数的期望可以表示为:

$$\begin{equation} \label{expform}
E(f(x,y)) = \sum_{x,y}{f(x,y)p(x,y)}
\end{equation}$$

经验期望和模型期望，就是分布从各自的方面去刻画$p(x,y)$。

对经验期望，认为$ p(x,y) = \tilde p(x,y) = \frac{Count(x,y)}{Count(all)} $

则($\ref{expform}$)从样本集分布的角度可以表示为：

$$\begin{equation} \label{empiricalexpform}
E_{\tilde p}(f(x,y)) = \sum_{x,y}{f(x,y) \tilde p(x,y)}
\end{equation}$$

对模型期望，认为$ p(x,y) = \tilde p(x) p(y\|x) = \frac{Count(x)}{Count(all)} p(y\|x)$

则($\ref{expform}$)从模型预测的角度可以表示为：

$$\begin{equation} \label{modelexpform}
E_{p}(f(x,y)) = \sum_{x,y}{f(x,y) \tilde p(x) p(y|x)}
\end{equation}$$

故($\ref{expequal}$)可表示为

$$\begin{equation} \label{expequaldetail}
\sum_{x,y}{f(x,y) \tilde p(x,y)} = \sum_{x,y}{f(x,y) \tilde p(x) p(y|x)}
\end{equation}$$

最后再表述一遍其包含的含义：经验期望表示了训练样本的统计现象的内在属性，同时也要求由模型得到的模型期望能够完整地展现这些统计现象的内在属性。

**其他约束**

 除了上述特征函数期望的约束外，还有从概率意义上的约束，即要求：

 $$\begin{equation} \label{probabilityconstraint}
\sum_{y}p(y|x) = 1
 \end{equation}$$

####最大熵模型的学习<sup><a href="ref3">[3]</a></sup>

*最大熵模型的学习问题就是最大熵模型的求解问题*。而最大熵模型求解可以形式化为**约束最优化问题**（线性约束下的非线性函数最优化问题）。

给定训练集$\\{ (x\_1 , y\_1 ) , (x\_2 , y\_2) , \cdots , (x\_N , y\_N)\\}$，以及特征函数$f\_i(x,y)$ , $i = 1 , 2 , \cdots , m $ ，最大熵模型的学习等价于约束最优化问题。

$$\begin{equation}
&max_{p \in C} &H(Y|X) = -\sum_{x,y} \tilde p(x) p(y|x) \log(p(y|x)) \\
&s.t. & E_{\tilde p}(f(x,y)) = E_p(f(x,y)) , i=1,2,\cdots,m \\
& &\sum_yp(y|x) = 1
\end{equation}$$

**按照最优化问题的习惯，我们将求最大值问题改写为等价的求最小化问题**(第一步转化)：

$$\begin{gather}
\label{target} &min_{p \in C} &-H(Y|X) = \sum_{x,y} \tilde p(x) p(y|x) \log(p(y|x)) \\
\label{expconstrain} &s.t. & E_{\tilde p}(f(x,y)) = E_p(f(x,y)) , i=1,2,\cdots,m \\
\label{probconstrain} & &\sum_yp(y|x) = 1
\end{gather}$$

到此，最大熵模型的学习问题就转化为了求解($\ref{target}$)($\ref{expconstrain}$)($\ref{probconstrain}$)这一约束条件下的最优化问题。该最优化问题的解，就是最大熵模型学习的解。

**我们要将约束下的最优化问题转化为无约束最优化的对偶问题，通过求解对偶问题来求解原始问题。**

*首先*，引入拉格朗日乘子（Lagrange）$w\_0 , w\_1 , w\_1 , \cdots , w\_m$，定义拉格朗日函数$L(P,w)$ :

$$\begin{split}
L(P,w) &= -H(Y|X) + w_0(1 - \sum_y{p(y|x)}}) + \sum_{i=1}^{m}{w_m (\sum_{x,y}{E_{\tilde p}(f_i(x,y)) - E_{p}{f_i(x,y)}})} \\
&\sum_{x,y}\tilde p(x) p(y|x) \log(p(y|x)) + w_0(1 - \sum_y{p(y|x)}) \\
&+ \sum_{i=1}^{m}{w_i (\sum_{x,y}\tilde p(x,y) f_i(x,y) - \sum_{x,y}\tilde p(x) p(y|x) f_i(x,y))}
\end{split}$$

定义拉格朗日函数后，则原始约束条件下最优化问题可转化为无约束最优化问题：

$$\begin{equation} \label{originprob}
min_{p \in C}max_{w}L(P,w)
\end{equation}$$

($\ref{originprob}$)称为最优化的*原始问题*。

又由于原始优化目标函数$H(Y|X)$是凸函数，原始问题可以转化为其*对偶问题*，即把极小极大交换位置，变为极大极小：

$$\begin{equation} \label{dualprob}
max_{w}min{p \in C}L(P,w)
\end{equation}$$

*接着*，我们先求解对偶问题($\ref{dualprob}$)中极小项，即$min_{p \in C}L(P,w)$。该函数是关于$w$的函数<sup>*</sup>，记为：

$$\begin{equation} \label{dualfunc}
\Psi(w) = min_{p \in C}L(P,w)
\end{equation}$$

将$\Psi(w)$称为*对偶函数*，其解记为

$$\begin{equation} \label{dualfuncsolve}
P_{w} = \arg_{p \in C} \min L(P,w) = P_{w}(y|x)
\end{equation}$$

> 为什么$\Psi(w)$是关于w的函数？ $L(P,w)$本是$P$（即$p(y\|x)$）和$w$的函数，但$min_{p \in C}$运算相当于求出了$P$的值，将其转为仅与$w$有关，只有经过这样的处理，外层的$max\_{w}$才有合理。


很明显，对$P$($p(y\|x)$)求偏导，并令导数为0即可求出该关于w的函数$P_{w}$ 。

$$\begin{split}
\frac{\partial L(P,w)}{\partial p(y|x)} &= \sum_{x,y}\tilde p(x) (\log p(y|x) + 1) - \sum_y{w_0} - \sum_{x,y}{\tilde p(x) \sum_{i=1}^{m}w_if_i(x,y)} \\
&=\sum_{x,y}\tilde p(x) (\log p(y|x) + 1 - w_0 - \sum_{i=1}^{m}w_i f_{i}(x,y))
\end{split}$$

> 注意其中的一些技巧： 为什么可以把$$

<span id="ref1">[1]</span> [最大熵模型中的数学推导](http://blog.csdn.net/v_july_v/article/details/40508465)

<span id="ref2">[2]</span> [条件随机场综述](http://www.paper.edu.cn/download/downPaper/201001-479)

<span id="ref3">[3]</span> 李航 · 《统计学习方法》

<span id="ref4">[4]</span> [维基百科](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))





