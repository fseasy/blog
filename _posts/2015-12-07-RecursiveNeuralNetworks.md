---
layout: post
title: Parsing natural Scene and natural Language with Recursive Neural Networks 论文阅读
date: 2015-12-7
categories: 技术 
tags: Recursive
onewords: 就 《Parsing natural Scene and natural Language with Recursive Neural Networks》 谈 Recursive Neural Networks
---
> 因为准备深度学习小组本次关于Recursive Neural Networks（在不导致歧义的情况下，后文均将其简称为RNNs，）的主题，阅读了这篇关于使用RNNs做图片切分、标注以及自然语言结构分析的文章。

RNNs处理的是存在结构递归的任务（而Recurrent Neural Network则是时间上的递归），通常应用于计算机视觉领域的场景理解（切分与标记），场景分类（如分类图片所处场景是城市、乡村或是海滨），以及自然语言的语法树构建。

上述任务本身都存在着“递归”的结构。图片可以使用嵌套的结构化表示，各元素间可以表示为“构成”与“相邻”的情况。；如论文上的例子，一张汽车停在公路上的图片，可以使用递归的形式来表示。个人给出一种表示，如 图片-> 子图+汽车 ；汽车->汽车部件+轮子 ；汽车部件->汽车部件+窗户 等等。这只是个人定义的方式，作为嵌套（递归）表示的一种可能。语法树可以递归表示应该是广为接受的，CFG文法应该可以看做一个的例子。

RNNs需要做的，主要包含3个工作：

1.  表示两个部件(segments)合并后的表示形式

2.  对合并后的表示形式给出分数

3.  对合并后的表示形式给出标签预测

以下再具体阐述RNNs的前馈过程：

1.  原始输入向量的转换

    如果原始输入是图片（矩阵），那么使用前人的方法，将图片分为K份，每份抽取119个特征（包含颜色、外观、形状等），用F\_i来表示。让F\_i特征向量经过一个隐层和激活函数，得到n维的向量。这样一个图片就表示为了K个n维空间中的向量。形式化的表示，F\_i 维度为 119 * 1 ，隐层权值W\_sem 为 n*119 , b 为n维向量，则 a\_i = f( W\_sem · F\_i + b) ，其中f函数这里就去sigmoid函数。得到的向量称为激活向量，维度为 n * 1 。
    如果原始输入是句子，那么首先句子分词，设有K个词，将每个词表示为One-Hot，然后再从WordEmbeding中取出对应的列，就得到了RNNs的n维向量输入。形式化的说，使用维度为\|V\| * 1 的e\_k 向量表示原始句子的词，其中第k行为1，其余为0；WordEmbeding表为L ， 维度为n\*\|V\|，那么L 与 e\_k的内积结果就是该词的Embeding表示，即RNNs的输入，维度为n\*1 。

2.  基于贪心方法递归构建树
    
    输入除了经过上述转换后的向量X，还有每个部件的邻接关系，即K*K的邻接矩阵。这里其实可以将其看做一个图，即由K个顶点，邻接矩阵定义的边构成的图。

    按照以下步骤生成树结构、预测标签。

    1.  首先按照邻接矩阵生成所有可能的两两组合对。将待组合的两个部件首尾连接起来，构成维度为2n*1的向量（矩阵）。由于连接有先后，所以a1与a2的连接，以及a2与a1的连接是不同的。
    
    2.  使用RNNs首先将上述组合变换为n\*1的向量，作为组合后的向量表示。该向量认为是两个原始部件的父亲节点。然而对转换后的向量打分。取分数最好的组合结果作为此次合并的结果。形式化表示为：p = f(w[ai;aj] + b) ; score = W\_score\*p .得到最高分的组合后，从组合中删去[ai , aj]， [aj , ai]，并且将其他组合中含有ai、aj的项全部替换为p；由此对应的就生成了一个以p为根，ai，aj为孩子节点的子树；对p向量再过一个softmax层，分类出其所属的标签。

3.  重复上述过程，直到只有一个组合。

由此，即完成了RNNs的前馈过程。


