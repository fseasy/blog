---
layout: post
title: 视觉里程计 | 视觉SLAM十四讲-ch7
date: 2022-03-28
categories: 技术 
tags: 视觉SLAM十四讲 SLAM 视觉里程计 odomotry
---
> 本讲开始，进入到实践应用部分。这节讲基于特征法的视觉里程计。

## 1 术语表

| 中文名称 | 英文名称                                              |
| :------- | :---------------------------------------------------- |
| 视觉里程计 | Visual Odometry, VO                                  |
| 关键点    | Key Point, kp |
| 描述子    | Descriptor |
| 角点      | corner |
| 矩       | moment, Image Moment (图像矩) |
| 灰度(值)  | intensity |
| 对极几何 | epipolar geometry                              |
| 极平面, 极点, 极线 | epipolar plane, epipoles, epipolar line       |
| 本质矩阵、本征矩阵 | Essential matrix, $\boldsymbol{\mathrm{E}}$ |

## 2 视觉里程计概述

视觉里程计，根据相邻图像的信息估计出粗略的相机运动。如果有后端，可给后端提供较好的初始值。

算法可分为两大类：*特征点法*和*直接法*； 特征点法先抽特征，具有稳定，对光照、动态物体不敏感的优势，但耗时大；
直接法则直接基于像素，少了特征提取步骤，耗时有所改善。

基于特征点法的视觉里程计，分 2 个步骤： 
1. 提取特征，找到匹配的点 
2. 基于匹配的点，计算相机的运动（里程计） 

## 3 步骤一：提特征 & 匹配

### 3.1 概述

首先看什么是特征点？特征点就是图里比较“特别”、关键的点（像素）， 一般要具备如下性质：

1. **可重复性 (Repeatability)**: 相同的特征，在两张图中都能找到。没有可重复性，源头上就没法做匹配。
2. **可区别性 (Distinctiveness)**: 不同特征有不同表达。 这个很显然，不可区分，则无法找到唯一匹配。
3. **高效率 (Efficiency)**: 同一张图，特征点数量要远少于像素量。 （难道不是指提特征速度快吗？233）
4. **本地性 (Locality)**: 特征仅与一小片区域有关。（就是局部特征吧，难道不可以提全局特征吗？233）

如何匹配？先要把提出的特征，用可度量的形式表示出来，然后用相应的度量函数计算特征间相似度/距离，最相似/距离最近的，就是匹配的特征点。
一般来说，特征都被表示为数值型向量；如果数值类型取浮点数，那么通常就用余弦相似度来做度量；而如果是 0/1 向量，就用汉明距离。

整个提特征的实际流程概述如下：

1. 从图里提特征，每个特征计算2个关键信息：关键点、描述子。关键点是特征点在图像中的位置，还可能包括朝向、大小信息；描述子是特征表示，即向量。
2. 对两张图里提出的所有特征，逐个基于描述子找匹配的特征，经过一定过滤后，得到一个关键点对集合 $\\{(kp_{1i}, kp_{2i}) \vert i=0,1,\cdots \\}$.

下面具体展开。

### 3.2 ORB 方法提特征

书上列了几个特征提取方法，如：

1. FAST角点法, 只能检测角点, 特征不稳定；
2. SIFT(尺度不变特征变换，Scale-Invariant Feature Transform). SIFT效果很好，但是速度慢，在SLAM里目前还不实用。
3. ORB（Oriented FAST and Rotated BRIEF）, 质量和性能的折衷，在 SLAM 中比较有优势（大名鼎鼎的 ORB-SLAM 就基于此，且以此命名)。

   > 在 ORB-SLAM 论文中写到: *Use of the same features for all tasks: tracking, mapping, relocalization and loop closing. This makes our system more efficient, simple and reliable. We use ORB features which allow real-time performance without GPUs, providing good invariance to changes in viewpoint and illumination.*

下面就主要介绍 ORB 方法。如名字（再写一遍， **O**riented FAST and **R**otated **B**RIEF, 话说缩写里根本不含 FAST ），它融合了 FAST 和 BRIEF 方法，并分别做改进。

首先是 Oriented FAST 抽关键点。原始 FAST 只判断目标像素是否是一个角点，核心原理是在像素粒度，检测周围是否至少有N个像素和当前像素的亮度有明显差异；如果有，那这个位置就是角点。显然 N 值对检测灵敏度很关键，因此 N 一般直接放到 FAST 方法名里，称为 FAST-N.
N 的常见值为9, 11, 12. 以 FAST-12 为例，整体流程如下：

<div class="post-image">
  <img src="/assets/posts/vslam/14-ch7/fast-12-r3-circle.webp" alt="fast-12 radius-3 circle range"
      width="200px">
  <p>图1： FAST-12 方法中半径为 3 的圆在图片像素上的实际范围及编号</p>
</div>

1. **“有差异”判定**：设中心像素亮度为 $I$，“差异”阈值为 $T = I * 0.2$, 则周围像素亮度 $p > I + T$ 或 $p < I - T$ 时，就认为二者亮度有差异
2. **比较范围选定**：以此位置为中心，选取半径为 3 的圆上的16个像素点，顺时针编号为 1~16（如图1）
3. **快筛**：若上下左右4个像素（序号为1、5、9、13）比较，至少3个点有差异（按概率估算，$4 \times \frac{12}{16} = 3$），才继续下面判断；否则直接认为此像素非角点。
4. **完整判断**：判断剩余12个像素。若总共有差异像素个数大于等于12个，则此像素为角点；否则不是。

可以看到 FAST-12 规则是启发式的，比较简单。简单让它高效，但必然有缺陷，概括起来是“分布不均匀，重复性不强”。
具体体现和相应的改进方法如下：

| <center>缺陷</center>  | <center>解决方案</center> |
|------|--------|
| 角点扎堆，很集中 | 增加非极大值抑制（Non-maximal suppresion） |
| 角点无方向，导致描述子不具备**旋转不变性** | 通过灰度质心法 (Intensity Centroid) 计算方向 |
| 取固定半径，导致特征无**尺度不变性**：远看像角点，近看就不是角点了 | 构建图像金字塔 |

**非极大值抑制**，即在一定区域内仅保留响应值极（最）大的角点。

**灰度质心法**，就是以特征点为中心 $O$，选一块矩形(一般正方形）区域。计算矩形区域内灰度质心点 $C$, 则 $\overrightarrow{OC}$ 便是此特征点的（旋转）方向。有此方向，就可以实现旋转不变性。

这里，我们显然要问，为啥这就行呢？这其实可以展开成 2 个问题， 1. 为什么计算旋转角度可得到旋转不变性？ 2. 为什么可以用灰度质心法计算角度？

<div class="post-image">
  <img src="/assets/posts/vslam/14-ch7/num2.webp" alt="number 2"
      width="300px">
  <img src="/assets/posts/vslam/14-ch7/num2-rotated.webp" alt="rotated number 2"
      width="300px">
  <p>图2： 正立的“2”和顺时针旋转45度的“2”</p>
  <p>从“2”的一横我们很容易看到旋转的角度；但是，如果我们用灰度质心法计算角度，却发现二者基本没有差别（前者-133.97 度，后者 -134.61 度）。这似乎说明灰度质心法计算旋转角度是有一定局限性的。</p>
</div>

我们以一个写着“2”的图片来做引子。
想想这个有 2 的图片，第一个时刻2是正的（旋转0度），第二个时刻2是斜着的（假设旋转45度），我们怎么让旋转前后的两个“2”匹配上呢（也就是旋转不变）？显然，我们可以先算出每个时刻2在图片上的旋转角度，然后在匹配前先把2反向旋转回去，这样就好匹配了！这回答了为什么计算旋转角度可得旋转不变。

接着再看为何可以用灰度质心法算角度。先从 [orb][^1] 原始论文看，该方法引自 [Measuring corner properties][^2]. 在此文中，计算 corner 的 orientation (方向) 有两种方法：灰度质心法、梯度质心法。orb 直接用了灰度质心法，而且把原文基于背景色的角度调整去掉了。没有太多的解释，我们只得去意会：把一块图像看做一个方形物体，图像的像素强度就是物体在每个位置的质量。像素强度不同，则图像质心不在中心。把图像旋转，则质心相应旋转，且转过的角度，就是图像中心和质心之间的直线（向量）转过的角度。

[^1]: ORB: An efficient alternative to SIFT or SURF, 2011, ICCV
[^2]: Measuring corner properties, 1996

书上是从图像矩的角度来计算灰度质心的：

$$\begin{align}
M_{pq} &= \sum_{x} \sum_{y} x^{p}y^{q} \mathop{I}(x, y) \\
C &= \left (\frac{M_{10}}{M_{00}}, \frac{M_{01}}{M_{00}} \right )
\end{align}
\tag{1. 基于图像矩求解灰度质心}
$$

其中 $x, y$ 是图像中各个像素的坐标（这时一般将图像中心作为坐标原点），$\mathop{I}(x, y)$ 是此坐标处的像素灰度值,
$C$ 是灰度质心坐标。 

我们也可以从普通物体计算质心的公式来推导上面的结果：
根据 [Center of mass][center_of_mass] 中 definition 部分，物体质心坐标为  

$$ C = \frac{1}{M}\sum_{i=1}^{n} m_i r_i $$

其中，$n$ 是质点个数，$m_i$ 是质点$i$的质量， $r_i$ 是相应坐标，$M = \sum_{i=1}^{n} m_i$ 表示所有质点质量（值的）和。
如前面所说，我们认为每个像素就是质点，其灰度值就是质量，坐标用$(x, y)$ 二维直角坐标系表示，则带入此质心坐标计算公式，就得到了前面的基于图像矩计算的公式。

看完书上和论文的描述，我自己也试验了下灰度质心法算旋转角度。如图2，我们看到灰度质心法算出的角度非常不靠谱。简单思考下，灰度质心法显然是有局限性的——极端情况下，假设图片就是1块纯白色，那不管我们旋转多少度，灰度质心法算出的两张图片旋转角度的差一定是0，因为全白图片里的像素并不会随旋转而改变。进一步的，灰度质心法要有效，前提是图片的灰度分布要非常不均匀，质心要偏离中心才行。我自己试了下左边黑、右边白的大色块，灰度质心法算出的旋转角度就非常精准。

**尺度不变性**，通用做法就是用金字塔法(Pyramid).

前面通过对 FAST 优化方法, 得到了关键点信息。下面就要计算关键点的描述子了。ORB 用的是 Rotated BRIEF 来计算描述子。

BRIEF, 即 Binary Robust Independent Elementary Features. 他对图片里的一批点对，用简单的布尔测试，得到一个表示，这些表示对 *lighting, blur, and perspective distortion* 都能得到比较鲁棒的结果，但对 *in-plane rotation* 处理不太好。所以 ORB 这里就增加了前面关键点得到的方向信息，也即 Rotated BRIEF, 从而优化了此问题。

具体地，Rotated BRIEF 

1. 从图片块中共选择 256 个点对 $P = \{(p_l, p_r), \cdots \}, \vert P \vert = 256$
2. 对每个点对，比较灰度值从而得到对应的特征值： $feat = 1 if I(p_l) < I(p_r) else 0$
3. 最终得到 256 维的布尔向量
 


<!-- **ORB方法**: 分别改进了 FAST角点法做特征点选择 和 BRIEF(Binary Robust Independent Elementary 
Feature). 原始FAST方法比较中心点和周围半径为3的点的亮度，如果亮度差异>T的个数超过N个，就认为是一个特征点；
改进的FAST称为 Orinted FAST: 针对其无尺度、
方向性问题，分别增加图像金字塔和灰度质心法。BRIEF是二进制描述子，通过对每个关键点附近随机取两个点，
比较亮度关系（大为1，小为0），得到一串0、1构成的向量作为描述子。其不具备旋转不变性。改进的BRIEF称为
Steer BRIEF: ORB结合Oriented FAST中获取的方向信息，计算旋转后的结果，因此具备良好的旋转不变性。

有了描述子之后，就可以基于描述子对关键点做匹配。描述子是向量，所以这块就是向量最近邻查找，就和 ANN 算法关联上了。
这块算比较成熟了，调包即可。书上说的库是 FLANN, 我所知常用的还有 Faiss, Annoy, Hnswlib 等。 -->

## 4 计算相机运动

基于前后时刻两帧的一组特征匹配点，我们可以用来估计相机的运动，即计算 $R$, $t$, $P_w$.

根据点的类型，估计的方法可以分为对极几何(2D-2D), ICP(3D-3D)，PnP(3D-2D).

### 4.1 对极几何

给定图片1、图片2和两个图片中点的匹配信息 $\left \\{(p_{1i}, p_{2i})\; \vert\; i = 1, 2, \cdots \right \\}$，可以求解

1. 图片1到图片2的坐标变换 $R$, $t$
2. 点深度

> 如果图片1、图片2是单目前后两帧的图片，则 $R$, $t$ 就是相机的运动。如果图片1、图片2是双目相机的左右图片，则主要得到解物体的深度。

一种漂亮的求解方法，就是对极几何。其定义了同一个物体 $P_c$ 和两个相机的光心 $O_1$, $O_2$、归一化成像平面上的像 $P_{n1}$, $P_{n2}$ 构成的平面。基于这个空间平面的性质，对上述变量有如下约束：

$\newcommand{\bs}{\boldsymbol}$

$$ \rm P_{n1}^T E P_{n2} = 0 \tag{1. 对极约束}$$

其中 

$$ \rm E = \bs{t}^\wedge R \tag{2. 本质矩阵}$$

被称为本质矩阵、本征矩阵。$ \bs{t}^\wedge $ 是平移向量 $\bs t$ 的反对称矩阵（3rd有讲）。

建议看 [从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?][z21_eg], 更轻松理解对极几何和对极约束。

[z21_eg]: https://mp.weixin.qq.com/s?__biz=MzIxOTczOTM4NA==&mid=2247486151&idx=1&sn=2b322f466d916704b1070ece20e669db&chksm=97d7ef50a0a06646a984fcbf82870011ec10a9233899ee74fe8c09432517c5efaa285f1897c9&token=116551560&lang=zh_CN#rd "从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?"

[image_moment]: https://en.wikipedia.org/wiki/Image_moment
[center_of_mass]: https://en.wikipedia.org/wiki/Center_of_mass
