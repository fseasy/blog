---
layout: post
title: 视觉里程计 | 视觉SLAM十四讲-ch7
date: 2022-03-28
categories: 技术 
tags: 视觉SLAM十四讲 SLAM 视觉里程计 odomotry
---
> 本讲开始，进入到实践应用部分。这节讲基于特征法的视觉里程计。

## 1 术语表

| 中文名称 | 英文名称                                              |
| :------- | :---------------------------------------------------- |
| 视觉里程计 | Visual Odometry, VO                                  |
| 关键点    | Key Point, kp |
| 描述子    | Descriptor |
| 角点      | corner |
| 矩       | moment, Image Moment (图像矩) |
| 灰度(值)  | intensity |
| 对极几何 | epipolar geometry                              |
| 极平面, 极点, 极线 | epipolar plane, epipoles, epipolar line       |
| 本质矩阵、本征矩阵 | Essential matrix, $\boldsymbol{\mathrm{E}}$ |

## 2 视觉里程计概述

视觉里程计，根据相邻图像的信息估计出粗略的相机运动。如果有后端，可给后端提供较好的初始值。

算法可分为两大类：*特征点法*和*直接法*； 特征点法先抽特征，具有稳定，对光照、动态物体不敏感的优势，但耗时大；
直接法则直接基于像素，少了特征提取步骤，耗时有所改善。

基于特征点法的视觉里程计，分 2 个步骤： 
1. 提取特征，找到匹配的点 
2. 基于匹配的点，计算相机的运动（里程计） 

## 3 步骤一：提特征 & 匹配

### 3.1 概述

首先看什么是特征点？特征点就是图里比较“特别”、关键的点（像素）， 一般要具备如下性质：

1. **可重复性 (Repeatability)**: 相同的特征，在两张图中都能找到。没有可重复性，源头上就没法做匹配。
2. **可区别性 (Distinctiveness)**: 不同特征有不同表达。 这个很显然，不可区分，则无法找到唯一匹配。
3. **高效率 (Efficiency)**: 同一张图，特征点数量要远少于像素量。 （难道不是指提特征速度快吗？233）
4. **本地性 (Locality)**: 特征仅与一小片区域有关。（就是局部特征吧，难道不可以提全局特征吗？233）

如何匹配？先要把提出的特征，用可度量的形式表示出来，然后用相应的度量函数计算特征间相似度/距离，最相似/距离最近的，就是匹配的特征点。
一般来说，特征都被表示为数值型向量；如果数值类型取浮点数，那么通常就用余弦相似度来做度量；而如果是 0/1 向量，就用汉明距离。

整个提特征的实际流程概述如下：

1. 从图里提特征，每个特征计算2个关键信息：关键点、描述子。关键点是特征点在图像中的位置，还可能包括朝向、大小信息；描述子是特征表示，即向量。
2. 对两张图里提出的所有特征，逐个基于描述子找匹配的特征，经过一定过滤后，得到一个关键点对集合 $\\{(kp_{1i}, kp_{2i}) \vert i=0,1,\cdots \\}$.

下面具体展开。

### 3.2 ORB 方法提特征

书上列了几个特征提取方法，如：

1. FAST(Features from Accelerated Segment Test)角点法, 只能检测角点, 特征不稳定
2. SIFT(尺度不变特征变换，Scale-Invariant Feature Transform), 效果好，但速度慢，在SLAM里目前还不实用
3. ORB(Oriented FAST and Rotated BRIEF), 质量和性能的折衷，在 SLAM 中比较有优势（大名鼎鼎的 ORB-SLAM 就基于此，且以此命名)。

   > 在 ORB-SLAM 论文中写到: *Use of the same features for all tasks: tracking, mapping, relocalization and loop closing. This makes our system more efficient, simple and reliable. We use ORB features which allow real-time performance without GPUs, providing good invariance to changes in viewpoint and illumination.*

下面就主要介绍 ORB 方法。如名字（再写一遍， **O**riented FAST and **R**otated **B**RIEF），它融合了 FAST 和 BRIEF 方法，并分别做改进。

首先是 Oriented FAST 抽关键点。原始 FAST 只判断目标像素是否是一个角点，核心原理是在像素粒度，检测周围是否有一定数量的**连续**像素与中心像素有**一致性**的亮度差异。同时，为了解决角点聚集的问题，使用非极大值抑制(nonmax-suppression)来保证最有差异的点作为角点。

> 论文里的完整 FAST 算法，可以参见 [Fast OpenCV Tutorial][fast_opencv_tutorial] 的描述。OpenCV 抛弃了其中的机器学习部分，直接用按规则来选定的比较顺序。OpenCV FAST 源码解析，有兴趣可看[OpenCV FAST 角点检测算法 CPU 版本实现源码注解]({% post_url 2022-03-28-vslam14c-7th %})

FAST 简单高效，但有缺陷，概括起来是“分布不均匀，重复性不强”，具体体现和相应的改进方法如下：

| <center>缺陷</center>  | <center>解决方案</center> |
|------|--------|
| 角点无方向，导致描述子不具备**旋转不变性** | 通过灰度质心法 (Intensity Centroid) 计算方向 |
| 取固定半径，导致特征无**尺度不变性**：远看像角点，近看就不是角点了 | 构建图像金字塔 |

**灰度质心法**，就是以特征点为中心 $O$，取围着的一块矩形(一般正方形）区域。计算矩形区域内灰度质心点 $C$, 则 $\overrightarrow{OC}$ 便是此特征点的方向。有此方向，就可以实现旋转不变性。灰度质心法也被简写为IC. 书上基于图像矩来计算灰度质心：

$$\begin{align}
M_{pq} &= \sum_{x} \sum_{y} x^{p}y^{q} \mathop{I}(x, y) \\
C &= \left (\frac{M_{10}}{M_{00}}, \frac{M_{01}}{M_{00}} \right )
\end{align}
\tag{1. 基于图像矩求解灰度质心}
$$

其中 $x, y$ 是图像中各个像素的坐标（这时一般将图像中心作为坐标原点），$\mathop{I}(x, y)$ 是此坐标处的像素灰度值。

这里我们稍微展开下。我看到书上这段描述的时候，有两个疑惑点，其一是为什么用灰度质心法就可以实现旋转不变性，其二是图像矩是什么？为何可以据此计算灰度质心呢？下面记录下自己的学习、不可靠猜想的过程。

首先是为何 IC 可以实现旋转不变性。这其实可以进一步展开成 2 个问题：1. 为什么计算旋转角度可得到旋转不变性？ 2. 为什么可以用灰度质心法计算角度？

<div class="post-image">
  <img src="/assets/posts/vslam/14-ch7/num2.webp" alt="number 2"
      width="300px">
  <img src="/assets/posts/vslam/14-ch7/num2-rotated.webp" alt="rotated number 2"
      width="300px">
  <p>图1： 正立的“2”和顺时针旋转45度的“2”</p>
  <p>从“2”的一横我们很容易看到旋转的角度；但是，如果我们用灰度质心法计算角度，却发现二者基本没有差别（前者-133.97 度，后者 -134.61 度）。这似乎说明灰度质心法计算旋转角度是有一定局限性的。</p>
</div>

我们以一个写着“2”的图片举例。第一个时刻（第一张图）2是正的（旋转0度），第二个时刻2是斜着的（旋转了45度），我们怎么让两个时刻的“2”匹配上呢（也就是旋转不变）？显然，我们可以先算出每个时刻2的旋转角度，然后在匹配前都先把2反向旋转回去，这样得到两个时刻都是正的“2”，就好匹配了！这回答了为什么计算旋转角度可得旋转不变。这里的“旋转角度”，用更标准地说法，应该是 orientation (方向).
接着再看为何可以用 IC 算 orientation. 我们可以把一块图片看做一个方形饼干，图像上的每个像素就是构成饼干的小粒，而像素强度就是饼干粒的质量。这个图像五彩斑斓/黑白分明，对应着饼干里面不同位置有不同的材质（有的地方只有面粉，有的还有夹心、葡萄干）。因为饼干质量分布不均匀，所以它的质心不在中心点（当我们想要把饼干平稳放在指尖时，就不能去选中心位置），且只要我们不吃它，饼干质心点相对中心的方向不因为我们旋转饼干而发生变化。也即，中心到质心的连线，随整体旋转而旋转。回到图片，也就是说用灰度质心法找到 $\overrightarrow{OC}$， 就能表示整个图片的 orientation 了。其实去 orb[^1] 原始论文看，IC 引自*Measuring corner properties*[^2]. 在此文中，计算 corner 的 orientation 有两种方法：灰度质心法、梯度质心法。orb 选择了简单又高效的灰度质心法，甚至把原文基于背景色做角度调整的操作也去了。

[^1]: ORB: An efficient alternative to SIFT or SURF, 2011, ICCV
[^2]: Measuring corner properties, 1996

再看为何基于图像矩可以算灰度质心？我看了下 wikipedia 的 [image moment][image_moment], 感觉这是一个需要记忆的概念，不好理解。
不过既然是“质心”，那肯定也可以从普通物体计算质心的公式来推导上面的结果吧。
根据 [Center of mass][center_of_mass] 中 definition 部分，物体质心坐标为  

$$ C = \frac{1}{M}\sum_{i=1}^{n} m_i r_i $$

其中，$n$ 是质点个数，$m_i$ 是质点$i$的质量， $r_i$ 是相应坐标，$M = \sum_{i=1}^{n} m_i$ 表示所有质点质量（值的）和。
如前面所说，我们认为每个像素就是质点，其灰度值就是质量，坐标用$(x, y)$ 二维直角坐标系表示，则带入此质心坐标计算公式，就得到了前面的基于图像矩计算的公式。所以，如果理解不了图像矩，也可以从质心公式（这个更直观？）出发来看这个结果。

最后，我自己也试验了下灰度质心法算旋转角度。如图2，很不凑巧，灰度质心法在“2”的例子上效果不好。简单思考下，灰度质心法显然是有局限性的：

1. 它必须要求图片灰度分布不均匀、质心偏离中心才行，这样中心与质心的连线才会随旋转而改变。极端情况下，假设图片就是一块纯白色，那不管我们旋转多少度，灰度质心法算出的两张图片旋转角度的差一定是0，因为全白图片质心和中心重叠，无法计算角度。我又试了下左边黑、右边白的大色块，灰度质心法算出的旋转角度就非常精准。

2. 图片块像素的**量**的分布随旋转不能发生变化。这个在实际中影响较大——因为图片是方的，一旦让图片绕某个点旋转，则部分像素可能会从图片框里移除，新的像素（其他块）会补上来。这导致旋转前后像素都不同了，质心计算肯定就不准了。

当然，我的测试也很有问题——图片太大、整体灰度差异不显著。因为 ORB 的处理对象是 corner（小，且灰度变化显著）, 而且 SLAM 时两张图片间差异也较小，所以上述两个问题应该影响没那么大。

**尺度不变性**，通用做法就是用金字塔法(Pyramid). 即对原始图片进行不同级别地缩放，然后在不同缩放级别的结果上做 FAST 匹配。下面的图示结果显著说明了金字塔法的重要性！

<div class="post-image">
  <img src="/assets/posts/vslam/14-ch7/orb.level1.webp" alt="number 2 corner detect in level 1"
      width="300px">
  <img src="/assets/posts/vslam/14-ch7/num2-rotated.webp" alt="number 2 corner detect in level 4"
      width="300px">
  <p>图2： 无金字塔（左）和 4 层金字塔（右）下“数字2”图片的角点匹配效果</p>
  <p>可以看到，不使用金字塔，匹配的角点非常少！而 4 层的金字塔，人工认为的角点基本覆盖了。这说明 FAST 角点检测受尺度影响很大。</p>
</div>

前面通过对 FAST 优化方法, 得到了关键点信息。下面就要计算关键点的描述子了。ORB 用的是 Rotated BRIEF 来计算描述子。

BRIEF, 即 Binary Robust Independent Elementary Features. 他对图片里的一批点对，用简单的布尔测试，得到一个表示，这些表示对 *lighting, blur, and perspective distortion* 都能得到比较鲁棒的结果，但对 *in-plane rotation* 处理不太好。所以 ORB 这里就增加了前面关键点得到的方向信息，也即 Rotated BRIEF, 从而优化了此问题。

具体地，Rotated BRIEF 

1. 从图片块中共选择 256 个点对 $P = \{(p_l, p_r), \cdots \}, \vert P \vert = 256$
2. 对每个点对，比较灰度值从而得到对应的特征值： $feat = 1 if I(p_l) < I(p_r) else 0$
3. 最终得到 256 维的布尔向量
 

<!-- **ORB方法**: 分别改进了 FAST角点法做特征点选择 和 BRIEF(Binary Robust Independent Elementary 
Feature). 原始FAST方法比较中心点和周围半径为3的点的亮度，如果亮度差异>T的个数超过N个，就认为是一个特征点；
改进的FAST称为 Orinted FAST: 针对其无尺度、
方向性问题，分别增加图像金字塔和灰度质心法。BRIEF是二进制描述子，通过对每个关键点附近随机取两个点，
比较亮度关系（大为1，小为0），得到一串0、1构成的向量作为描述子。其不具备旋转不变性。改进的BRIEF称为
Steer BRIEF: ORB结合Oriented FAST中获取的方向信息，计算旋转后的结果，因此具备良好的旋转不变性。

有了描述子之后，就可以基于描述子对关键点做匹配。描述子是向量，所以这块就是向量最近邻查找，就和 ANN 算法关联上了。
这块算比较成熟了，调包即可。书上说的库是 FLANN, 我所知常用的还有 Faiss, Annoy, Hnswlib 等。 -->

## 4 计算相机运动

基于前后时刻两帧的一组特征匹配点，我们可以用来估计相机的运动，即计算 $R$, $t$, $P_w$.

根据点的类型，估计的方法可以分为对极几何(2D-2D), ICP(3D-3D)，PnP(3D-2D).

### 4.1 对极几何

给定图片1、图片2和两个图片中点的匹配信息 $\left \\{(p_{1i}, p_{2i})\; \vert\; i = 1, 2, \cdots \right \\}$，可以求解

1. 图片1到图片2的坐标变换 $R$, $t$
2. 点深度

> 如果图片1、图片2是单目前后两帧的图片，则 $R$, $t$ 就是相机的运动。如果图片1、图片2是双目相机的左右图片，则主要得到解物体的深度。

一种漂亮的求解方法，就是对极几何。其定义了同一个物体 $P_c$ 和两个相机的光心 $O_1$, $O_2$、归一化成像平面上的像 $P_{n1}$, $P_{n2}$ 构成的平面。基于这个空间平面的性质，对上述变量有如下约束：

$\newcommand{\bs}{\boldsymbol}$

$$ \rm P_{n1}^T E P_{n2} = 0 \tag{1. 对极约束}$$

其中 

$$ \rm E = \bs{t}^\wedge R \tag{2. 本质矩阵}$$

被称为本质矩阵、本征矩阵。$ \bs{t}^\wedge $ 是平移向量 $\bs t$ 的反对称矩阵（3rd有讲）。

建议看 [从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?][z21_eg], 更轻松理解对极几何和对极约束。

[z21_eg]: https://mp.weixin.qq.com/s?__biz=MzIxOTczOTM4NA==&mid=2247486151&idx=1&sn=2b322f466d916704b1070ece20e669db&chksm=97d7ef50a0a06646a984fcbf82870011ec10a9233899ee74fe8c09432517c5efaa285f1897c9&token=116551560&lang=zh_CN#rd "从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?"

[image_moment]: https://en.wikipedia.org/wiki/Image_moment
[center_of_mass]: https://en.wikipedia.org/wiki/Center_of_mass
[fast_opencv_tutorial]: https://docs.opencv.org/3.4/df/d0c/tutorial_py_fast.html "FAST Algorithm for Corner Detection"