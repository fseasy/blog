---
layout: post
title: 视觉里程计 | 视觉SLAM十四讲-ch7
date: 2022-03-28
categories: 技术 
tags: 视觉SLAM十四讲 SLAM 视觉里程计 odomotry
---
> 本讲开始，进入到实践应用部分。这节讲基于特征法的视觉里程计。

## 1 术语表

| 中文名称 | 英文名称                                              |
| :------- | :---------------------------------------------------- |
| 视觉里程计 | Visual Odometry, VO                                  |
| 关键点    | Key Point, kp |
| 描述子    | Descriptor |
| 角点      | corner |
| 矩       | moment, Image Moment (图像矩) |
| 灰度(值)  | intensity |
| 对极几何 | epipolar geometry                              |
| 极平面, 极点, 极线 | epipolar plane, epipoles, epipolar line       |
| 本质矩阵、本征矩阵 | Essential matrix, $\boldsymbol{\mathrm{E}}$ |

## 2 视觉里程计概述

视觉里程计，根据相邻图像的信息估计出粗略的相机运动。如果有后端，可给后端提供较好的初始值。

算法可分为两大类：*特征点法*和*直接法*； 特征点法先抽特征，具有稳定，对光照、动态物体不敏感的优势，但耗时大；
直接法则直接基于像素，少了特征提取步骤，耗时有所改善。

基于特征点法的视觉里程计，分 2 个步骤： 
1. 提取特征，找到匹配的点 
2. 基于匹配的点，计算相机的运动（里程计） 

## 3 步骤一：提特征 & 匹配

### 3.1 概述

首先看什么是特征点？特征点就是图里比较“特别”、关键的点（像素）， 一般要具备如下性质：

1. **可重复性 (Repeatability)**: 相同的特征，在两张图中都能找到。没有可重复性，源头上就没法做匹配。
2. **可区别性 (Distinctiveness)**: 不同特征有不同表达。 这个很显然，不可区分，则无法找到唯一匹配。
3. **高效率 (Efficiency)**: 同一张图，特征点数量要远少于像素量。 （难道不是指提特征速度快吗？233）
4. **本地性 (Locality)**: 特征仅与一小片区域有关。（就是局部特征吧，难道不可以提全局特征吗？233）

如何匹配？先要把提出的特征，用可度量的形式表示出来，然后用相应的度量函数计算特征间相似度/距离，最相似/距离最近的，就是匹配的特征点。
一般来说，特征都被表示为数值型向量；如果数值类型取浮点数，那么通常就用余弦相似度来做度量；而如果是 0/1 向量，就用汉明距离。

整个提特征的实际流程概述如下：

1. 从图里提特征，每个特征计算2个关键信息：关键点、描述子。关键点是特征点在图像中的位置，还可能包括朝向、大小信息；描述子是特征表示，即向量。
2. 对两张图里提出的所有特征，逐个基于描述子找匹配的特征，经过一定过滤后，得到一个关键点对集合 $\\{(kp_{1i}, kp_{2i}) \vert i=0,1,\cdots \\}$.

下面具体展开。

### 3.2 ORB 方法提特征

书上列了几个特征提取方法，如：

1. FAST角点法, 只能检测角点, 特征不稳定；
2. SIFT(尺度不变特征变换，Scale-Invariant Feature Transform). SIFT效果很好，但是速度慢，在SLAM里目前还不实用。
3. ORB（Oriented FAST and Rotated BRIEF）, 质量和性能的折衷，在 SLAM 中比较有优势（大名鼎鼎的 ORB-SLAM 就基于此，且以此命名)。

   > 在 ORB-SLAM 论文中写到: *Use of the same features for all tasks: tracking, mapping, relocalization and loop closing. This makes our system more efficient, simple and reliable. We use ORB features which allow real-time performance without GPUs, providing good invariance to changes in viewpoint and illumination.*

下面就主要介绍 ORB 方法。如名字（再写一遍， **O**riented FAST and **R**otated **B**RIEF, 话说缩写里根本不含 FAST ），它融合了 FAST 和 BRIEF 方法，并分别做改进。

首先是 Oriented FAST 抽关键点。原始 FAST 只判断目标像素是否是一个角点，核心原理是在像素粒度，检测周围是否至少有N个像素和当前像素的亮度有明显差异；如果有，那这个位置就是角点。显然 N 值对检测灵敏度很关键，因此 N 一般直接放到 FAST 方法名里，称为 FAST-N。
N 的常见值为9, 11, 12。以 FAST-12 为例，整体流程如下：

<div class="post-image">
  <img src="/assets/posts/vslam/14-ch7/fast-12-r3-circle.webp" alt="fast-12 radius-3 circle range"
      width="200px">
  <p>图1： FAST-12 方法中半径为 3 的圆在图片像素上的实际范围及编号</p>
</div>

1. **“有差异”判定**：设中心像素亮度为 $I$，“差异”阈值为 $T = I * 0.2$, 则周围像素亮度 $p > I + T$ 或 $p < I - T$ 时，就认为二者亮度有差异
2. **比较范围选定**：以此位置为中心，选取半径为 3 的圆上的16个像素点，顺时针编号为 1~16（如图1）
3. **快筛**：若上下左右4个像素（序号为1、5、9、13）比较，至少3个点有差异（按概率估算，$4 \times \frac{12}{16} = 3$），才继续下面判断；否则直接认为此像素非角点。
4. **完整判断**：判断剩余12个像素。若总共有差异像素个数大于等于12个，则此像素为角点；否则不是。

可以看到 FAST-12 规则是启发式的，比较简单。简单让它高效，但必然有缺陷，概括起来是“分布不均匀，重复性不强”。
具体体现和相应的改进方法如下：

| <center>缺陷</center>  | <center>解决方案</center> |
|------|--------|
| 角点扎堆，很集中 | 增加非极大值抑制（Non-maximal suppresion） |
| 角点无方向，导致描述子不具备**旋转不变性** | 通过灰度质心法 (Intensity Centroid) 计算方向 |
| 取固定半径，导致特征无**尺度不变性**：远看像角点，近看就不是角点了 | 构建图像金字塔 |

**非极大值抑制**，即在一定区域内仅保留响应值极（最）大的角点。

**灰度质心法**，就是以特征点为中心 $O$，选一块矩形(一般正方形）区域。计算矩形区域内灰度质心点 $C$, 则 $\vec{OC}$ 便是此特征点的（旋转）方向。有此方向，就可以实现旋转不变性。

这里，我们显然要问，为啥这就行呢？这其实可以展开成 2 个问题， 1. 为什么计算旋转角度可得到旋转不变性？ 2. 为什么用灰度质心法计算角度？

我们以一个写着“2”的图片来做引子。
想想这个有 2 的图片，第一个时刻2是正的（旋转0度），第二个时刻2是斜着的（假设旋转45度），我们怎么让旋转前后的两个“2”匹配上呢（也就是旋转不变）？显然，我们可以先算出每个时刻2在图片上的旋转角度，然后在匹配前先把2反向旋转回去，这样就好匹配了！这回答了为什么计算旋转角度可得旋转不变。

接着再看为何要用灰度质心法算角度。回到“2”的例子，怎么才知道2的旋转角度呢？最好找个简单又稳定的方法。对“2”而言，我们可以找“2”最后横的那一笔，它和x轴的角度，可以直观、稳定地表示2的旋转角度。不多对普通的图片，并没有特别的“横”，但我们知道原始物体的颜色（灰度）分布是不会随旋转改变的，所以它具备稳定性，而且颜色/灰度是任何图片都有的，有通用性；然而一堆颜色集合，怎么来算角度呢？ 答案就是灰度质心法——求物体的质心，一般要用到“矩”的概念，在图像上，就是图像矩。每个位置的灰度值，可以看做密度，分别沿 x、y 轴加权求和灰度值，就可得到质心坐标了。

在下一刻相机移动后，该特征点对应位置的像素维度的表示肯定变了，且 $\vec{OC}$ 也会变化，但综合这两个变化，却可以还原出不变的表示，从而达成旋转后依然可以找到匹配点的 “旋转不变性”。 


<!-- **ORB方法**: 分别改进了 FAST角点法做特征点选择 和 BRIEF(Binary Robust Independent Elementary 
Feature). 原始FAST方法比较中心点和周围半径为3的点的亮度，如果亮度差异>T的个数超过N个，就认为是一个特征点；
改进的FAST称为 Orinted FAST: 针对其无尺度、
方向性问题，分别增加图像金字塔和灰度质心法。BRIEF是二进制描述子，通过对每个关键点附近随机取两个点，
比较亮度关系（大为1，小为0），得到一串0、1构成的向量作为描述子。其不具备旋转不变性。改进的BRIEF称为
Steer BRIEF: ORB结合Oriented FAST中获取的方向信息，计算旋转后的结果，因此具备良好的旋转不变性。

有了描述子之后，就可以基于描述子对关键点做匹配。描述子是向量，所以这块就是向量最近邻查找，就和 ANN 算法关联上了。
这块算比较成熟了，调包即可。书上说的库是 FLANN, 我所知常用的还有 Faiss, Annoy, Hnswlib 等。 -->

## 4 计算相机运动

基于前后时刻两帧的一组特征匹配点，我们可以用来估计相机的运动，即计算 $R$, $t$, $P_w$.

根据点的类型，估计的方法可以分为对极几何(2D-2D), ICP(3D-3D)，PnP(3D-2D).

### 4.1 对极几何

给定图片1、图片2和两个图片中点的匹配信息 $\left \\{(p_{1i}, p_{2i})\; \vert\; i = 1, 2, \cdots \right \\}$，可以求解

1. 图片1到图片2的坐标变换 $R$, $t$
2. 点深度

> 如果图片1、图片2是单目前后两帧的图片，则 $R$, $t$ 就是相机的运动。如果图片1、图片2是双目相机的左右图片，则主要得到解物体的深度。

一种漂亮的求解方法，就是对极几何。其定义了同一个物体 $P_c$ 和两个相机的光心 $O_1$, $O_2$、归一化成像平面上的像 $P_{n1}$, $P_{n2}$ 构成的平面。基于这个空间平面的性质，对上述变量有如下约束：

$\newcommand{\bs}{\boldsymbol}$

$$ \rm P_{n1}^T E P_{n2} = 0 \tag{1. 对极约束}$$

其中 

$$ \rm E = \bs{t}^\wedge R \tag{2. 本质矩阵}$$

被称为本质矩阵、本征矩阵。$ \bs{t}^\wedge $ 是平移向量 $\bs t$ 的反对称矩阵（3rd有讲）。

建议看 [从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?][z21_eg], 更轻松理解对极几何和对极约束。

[z21_eg]: https://mp.weixin.qq.com/s?__biz=MzIxOTczOTM4NA==&mid=2247486151&idx=1&sn=2b322f466d916704b1070ece20e669db&chksm=97d7ef50a0a06646a984fcbf82870011ec10a9233899ee74fe8c09432517c5efaa285f1897c9&token=116551560&lang=zh_CN#rd "从零开始一起学习SLAM \| 不推公式，如何真正理解对极约束?"