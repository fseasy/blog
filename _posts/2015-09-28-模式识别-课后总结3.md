---
layout: mathpage
title: 模式识别-课后总结3
date: 2015-09-28
categories: 笔记
tags: 模式识别
onewords: 第三课主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM。
---
> 主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM 。

*写在最前面：笔记类总结几乎都是自己的总结，所以一些内容是根据自己的理解表达的，包含很多不规范的内容。在正式场合，这些内容不应该被作为定义或官方解释。* 


##参数估计

我们认为一个数据集中的样本都是满足一个分布的。参数估计就是已知一系列样本，去估计产生这些样本的分布模型。这是一个比较难的问题，因为我们不知道具体是什么分布，所以这个估计问题就显得无从下手了。后面介绍的最大似然估计（或者被称为极大似然估计）和贝叶斯估计，都对分布模型首先做了一定的假设，但它们的假设条件不尽相同，所以二者方法有一些差异。

###最大似然估计

训练集中有一批样本，在最大似然估计的条件下，首先假设这些样本满足一个确定类型的分布——即分布的类型已知（如正态分布、均匀分布），且该分布的参数是确定的值，只是这个值需要我们根据样本信息来求解。

由上面的定义，可知对于最大似然估计，首先是假设一个分布，有了这个假设分布后，对模型的估计就变为了对具体参数的估计。同时，又假设分布的参数仅仅是一个确定值，所以这就变为了一个求函数（关于分布参数的函数）最优解的问题。这是比较直观的数值解。

最大似然估计，要求得相应的分布参数，使得该参数确定的分布**最有可能产生该数据集**。形式化的表述，就是：

$$\theta = \arg \max_{\theta}P(D | \theta)$$

其中$\theta$是假设分布的参数，$D$是数据集。再假设数据集中的每个样本是独立的（独立抽取） , 则可以把$P(D | \theta)$展开为与每个样本相关的函数：

$$\theta = \arg \max_{\theta}\prod_{i=1}^{N} P(x_i | \theta)$$

$N$为数据集中的样本数量。

当然一般我们将右边的函数去自然对数，由于自然对数单调递增，对 $\arg \max_{\theta}$运算没有任何影响。而取对数，确带来很多好处——可计算，将较小的概率变为一个绝对值更大的负数，同时防止连乘导致数值下溢。

$$\theta = \arg \max_{\theta}\sum_{i=1}^{N}{\ln P(x_i | \theta)}$$

###贝叶斯估计

暂时没有看懂

###主成分分析（Principal Conponents Analysis , PCA）

用途：降维

原理：待补充

具体方法：

首先，将样本空间的样本集表示为矩阵的形式，一行表示一个实例，一列表示一维特征；

设有m个实例，n维特征，即样本空间表示为$m \times n$矩阵

1. 求各维特征的平均值，然后各维特征值减去对应维的平均值，再除以该维特征的标准差（归一化）

    假设对第x维特征，先求$\bar x$ , 然后求$ x_i - \bar x$ ， 求$\sigma = \frac {\sum\_i{x\_i - \bar x}}{n-1}$ ， 最后再归一化 $\frac{x\_i - \bar x}{\sigma}$

    结果$m \times n$

2. 经过上述处理后，接着算各维特征间的协方差

    协方差求法： $$
    \begin{align*}
    COV(x , y) &= E((x - \bar x)(y - \bar y)) \\
               &= \sum_{x , y}{(x-\bar x)(y - \bar y)p(x , y)} \\
               &= \frac{D(X+Y) - D(X) - D(Y)}{2}
    \end{align*}$$

    n个特征，得到$n \times n$的协方差矩阵

3. 求协方差矩阵的特征值和特征向量 