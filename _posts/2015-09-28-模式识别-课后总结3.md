---
layout: mathpage
title: 模式识别-课后总结3
date: 2015-09-28
categories: 笔记
tags: 模式识别
onewords: 第三课主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM。
---
> 主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM 。

*写在最前面：笔记类总结几乎都是自己的总结，所以一些内容是根据自己的理解表达的，包含很多不规范的内容。在正式场合，这些内容不应该被作为定义或官方解释。* 


##参数估计

我们认为一个数据集中的样本都是满足一个分布的。参数估计就是已知一系列样本，去估计产生这些样本的分布模型。这是一个比较难的问题，因为我们不知道具体是什么分布，所以这个估计问题就显得无从下手了。后面介绍的最大似然估计（或者被称为极大似然估计）和贝叶斯估计，都对分布模型首先做了一定的假设，但它们的假设条件不尽相同，所以二者方法有一些差异。

###最大似然估计

训练集中有一批样本，在最大似然估计的条件下，首先假设这些样本满足一个确定类型的分布——即分布的类型已知（如正态分布、均匀分布），且该分布的参数是确定的值，只是这个值需要我们根据样本信息来求解。

由上面的定义，可知对于最大似然估计，首先是假设一个分布，有了这个假设分布后，对模型的估计就变为了对具体参数的估计。同时，又假设分布的参数仅仅是一个确定值，所以这就变为了一个求函数（关于分布参数的函数）最优解的问题。这是比较直观的数值解。

最大似然估计，要求得相应的分布参数，使得该参数确定的分布**最有可能产生该数据集**。形式化的表述，就是：

$$\theta = \arg \max_{\theta}P(D | \theta)$$

其中$\theta$是假设分布的参数，$D$是数据集。再假设数据集中的每个样本是独立的（独立抽取） , 则可以把$P(D | \theta)$展开为与每个样本相关的函数：

$$\theta = \arg \max_{\theta}\prod_{i=1}^{N} P(x_i | \theta)$$

$N$为数据集中的样本数量。

当然一般我们将右边的函数去自然对数，由于自然对数单调递增，对 $\arg \max_{\theta}$运算没有任何影响。而取对数，确带来很多好处——可计算，将较小的概率变为一个绝对值更大的负数，同时防止连乘导致数值下溢。

$$\theta = \arg \max_{\theta}\sum_{i=1}^{N}{\ln P(x_i | \theta)}$$

###贝叶斯估计

暂时没有看懂

###主成分分析（Principal Conponents Analysis , PCA）

> 大部分内容来自[1](#anchor1)，如果内容特别相近，会直接以引用形式出现；否则即是在文章基础上做了个人的重组织。

用途：降维

原理：待补充

具体方法：

首先，将样本空间的样本集表示为矩阵的形式，一行表示一个实例，一列表示一维特征；

设有m个实例，n维特征，即样本空间表示为$m \times n$矩阵

1. 求各维特征的平均值，然后各维特征值减去对应维的平均值，再除以该维特征的标准差（归一化）

    假设对第x维特征，先求$\bar x$ , 然后求$ x_i - \bar x$ ， 求$\sigma = \frac {\sum\_i{x\_i - \bar x}}{n-1}$ ， 最后再归一化 $\frac{x\_i - \bar x}{\sigma}$

    结果$m \times n$

2. 经过上述处理后，接着算各维特征间的协方差

    协方差求法：

     $$
    \begin{align*}
    COV(x , y) &= E((x - \bar x)(y - \bar y)) \\
               &= \sum_{x , y}{(x-\bar x)(y - \bar y)p(x , y)} \\
               &= \frac{D(X+Y) - D(X) - D(Y)}{2}
    \end{align*}$$

    n个特征，得到$n \times n$的协方差矩阵

    > 两个特征间的协方差反映了两个特征间的关系：如果协方差大于0，则一个特征增，则另一个特征也增加；如果小于0，一个增加则另一个减少；如果协方差为0，则两者相互独立。<sup><a href="#anchor1">1</a></sup>

3. 求协方差矩阵的特征值和特征向量
    
    特征值与特征向量：

    *特征值定义在方阵的基础上* ， 如下定义：

    对于方阵$A$ , 如果存在*非零*向量$\vec x$和常数$\lambda$，满足 $A \vec x = \lambda \vec x$ ，则称$\labmda$是方阵$A$的一个特征值，$\vec x$为该方阵*对应特征值*$\lambda$的特征向量。

    对于$n \times n$的方阵，通常来说有n个特征值（可以相等），而每个特征值都对应一个特征向量（维度为 $n \times 1$）

    > **关于特征向量、特征值、方阵的理解： n阶方阵可以看做是n维空间上的一个实体，这个实体是在n个正交方向上有不同投影大小的物体。且，n个特征向量是方阵的n个正交向量，而对应的特征值是矩阵在该方向上的投影。**<sup><a href="#anchor2">[2]</a></sup>

    重复一遍，**特征值可以看做投影**。

4. 根据降维需要选取前k大的特征值，并将对应的特征向量合并作为变换矩阵

    选择前k大的特征值，并将对应的特征向量作为列向量排列，最终形成一个$n \times k$的矩阵，这个就是关键的变换矩阵。

    由前面的特征向量的理解可知，我们就是取了原来矩阵的部分要投影的正交向量，且在这些正交向量上的投影值在所有正交向量投影值中前k大的。——即是说，选择了在规定范围内最大的投影——主要成分得以保留。

5. 将原始样本空间中的样本集矩阵，乘上上面得到的变换矩阵，得到降维后的样本集

    原始样本矩阵$m \times n$ ， 变换矩阵$n \times k$ ， 二者相乘得到的结果为$m \times k$ ， 对每一个样本来说，便从n维降到了k维。

    我们知道向量的点积可以看作是一个向量在另一个向量上的投影。

    那么矩阵与向量的乘积呢，我们可以把矩阵看作m个横向量，那么矩阵与向量的乘积就可以看作m个向量在该向量上的投影。

    再扩展，矩阵与矩阵的乘积，可以看作m个横向量，与k个列向量的点积，即是一个向量在k个向量上的投影，合并为一个新的向量。这样理解就与上面的特征向量的理解比较吻合 -> 与变换矩阵（其实是投影较大的特征向量）的乘积可以看作是取在这些方向上的投影。

    > 第5点的理解是个人观点，不甚成熟




<span id="anchor1">[1]</span> [主成分分析（Principal components analysis）-最大方差解释](http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html)

<span id="anchor2">[2]</span> [我对特征值与特征向量的理解](http://zisong.me/post/wo-dui-te-zheng-zhi-yu-te-zheng-xiang-liang-de-li-jie)

