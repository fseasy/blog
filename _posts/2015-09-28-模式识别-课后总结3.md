---
layout: mathpage
title: 模式识别-课后总结3
date: 2015-09-28
categories: 笔记
tags: 模式识别
onewords: 第三课主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM。
---
> 主要讲最大似然估计、贝叶斯估计、PCA主成分分析、EM算法和HMM 。

*写在最前面：笔记类总结几乎都是自己的总结，所以一些内容是根据自己的理解表达的，包含很多不规范的内容。在正式场合，这些内容不应该被作为定义或官方解释。* 


##参数估计

我们认为一个数据集中的样本都是满足一个分布的。参数估计就是已知一系列样本，去估计产生这些样本的分布模型。这是一个比较难的问题，因为我们不知道具体是什么分布，所以这个估计问题就显得无从下手了。后面介绍的最大似然估计（或者被称为极大似然估计）和贝叶斯估计，都对分布模型首先做了一定的假设，但它们的假设条件不尽相同，所以二者方法有一些差异。

###最大似然估计

训练集中有一批样本，在最大似然估计的条件下，首先假设这些样本满足一个确定类型的分布——即分布的类型已知（如正态分布、均匀分布），且该分布的参数是确定的值，只是这个值需要我们根据样本信息来求解。

由上面的定义，可知对于最大似然估计，首先是假设一个分布，有了这个假设分布后，对模型的估计就变为了对具体参数的估计。同时，又假设分布的参数仅仅是一个确定值，所以这就变为了一个求函数（关于分布参数的函数）最优解的问题。这是比较直观的数值解。

最大似然估计，要求得相应的分布参数，使得该参数确定的分布**最有可能产生该数据集**。形式化的表述，就是：

$$\theta = \arg \max_{\theta}P(D | \theta)$$

其中$\theta$是假设分布的参数，$D$是数据集。再假设数据集中的每个样本是独立的（独立抽取） , 则可以把$P(D | \theta)$展开为与每个样本相关的函数：

$$\theta = \arg \max_{\theta}\prod_{i=1}^{N} P(x_i | \theta)$$

$N$为数据集中的样本数量。

当然一般我们将右边的函数去自然对数，由于自然对数单调递增，对 $\arg \max_{\theta}$运算没有任何影响。而取对数，确带来很多好处——可计算，将较小的概率变为一个绝对值更大的负数，同时防止连乘导致数值下溢。

$$\theta = \arg \max_{\theta}\sum_{i=1}^{N}{\ln P(x_i | \theta)}$$





