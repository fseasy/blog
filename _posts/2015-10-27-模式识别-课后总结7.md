---
layout: mathpage
title: 模式识别-课后总结7
date: 2015-10-27
categories: 笔记
tags: 模式识别
onewords: 第七课主要讲前馈神经网络相关的知识。
---
> 主要讲前馈神经网络中的相关知识，包括线性分类器的局限性、神经网络的表达能力、结构介绍、BP算法、加冲量。

> 编辑中

###线性分类的局限性

对于线性可分的样本集，我们使用线性分类器就能得到满意的结果。但面对线性不可分的样本集，基于线性的分类就不能适应了。我们就说线性分类器的表达能力仅能处理线性可分的数据集。

例如，对于*异或问题*，线性分类器就不能够解决。

在前面的课程中讲过，如果我们能够扩展样本表达所在的空间维度，即升维，那么原本线性不可分的样本，在高维空间中也许就能够线性可分了。我们定义一个函数F，其输入是当前空间维度下的样本表示，输出是高维空间下的样本表示。一个实例：

$$
F( (x_1 , x_2) ) = (x_1 , x_2 , \alpha x_1 x_2 )
$$

书上的一个例子，经过该空间变换，就可以在高维空间中找到一个线性函数来分开样本了。

以上就是线性分类器的局限性以及在该局限性下人们可能采取的方法——核函数。这也是SVM的一个特征。

在*A Course in Machine Learning *中似乎提到过，对于线性不可分的问题，一种方法是使用核函数映射到高维空间，另一种当然就是使用非线性的分类方法。神经网络就属于非线性的分类方法。

###神经网络的定义及其表达能力

（人工）神经网络(Artificial Neural Networks)这个称呼的由来，就说明了这是人们大脑仿生的一种尝试。

PPT上定义说，神经网络是多层感知器的组合。以前也这么理解，不过上次看了[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)里*sigmoid 神经元*一章，也接受了感知器与sigmoid神经元不是一个东西这么一个概念。虽说很多东西都是很像的，它们产生时往往都是很相关的，不过我们后来去定义它们的时候，为了规范化以及消歧，往往就限定了其含义。我们这里也理解为：

神经网络是多层sigmoid神经元的组合。每层之间的节点是全连接的。

sigmoid神经元对输入做了非线性变换，且多层次连接，最终得到一个非线性的分类界面。其表达能力很强（高适应性 , high flexibility）。

以下贴两个PPT上的图来形象地说明神经网络是如何拥有高适应的表达能力的：

![XOR 异或表达](/assets/img/class/pattern_recognition/pattern_recognition_6_1.jpg)

![如何生成非线性分类面](/assets/img/class/pattern_recognition/pattern_recognition_6_2.jpg)

第一个图用一个简单的3层神经网络完成了异或的表达；

第二图则形象地展示了如何从一个个简单的分类面通过非线性组合形成最终的复杂非线性分类界面。

我们注意到单纯的一个sigmoid神经元，在空间产生的是一个sigmoid形状（即S型）的分类面。通过下一层（隐层或输出层，图中是输出层）的非线性变换（线性组合，加以非线性激活函数），变为了一个非线性分类界面。



