---
layout: mathpage
title: 模式识别-课后总结4
date: 2015-10-19
categories: 笔记
tags: 模式识别
onewords: 第四课主要讲非参数方法，包含parzen窗，k近邻方法。
---
> 主要讲非参数估计的技术，包含parzen窗，k近邻方法 。

> 编辑中

之前讲的贝叶斯决策理论、最大似然估计等都是在概率密度（分布函数）已知（知道先验知识，似然函数等信息 ，或者知道满足某种具体的分布）的情况下进行估计、预测。本章讲的内容则是在概率密度函数未知的情况下去做估计。

直观的想法是，我们先根据样本去估计一个概率密度分布，然后用该概率密度去进行预测。

最直观的用样本去估计概率密度的方法是直方图估计。就是把样本的分布按照某个区间划分为直方图表示，然后用这个表示概率密度分布。这个在数据比较稀疏、连续时往往不是很好表示，特别是针对高维数据，有点不是很好弄。（对了，决策树是不是就是可以这么来看呢？）

所以这章主要介绍了广泛适用的parzen窗和k近邻方法。

###parzen窗

parzen窗是用来做概率密度估计的。


**注意注意！**parzen窗方法也叫作`Kernel Density Estimation` ， 不同领域叫法不同。<sup><a href="#a1">[1]</a></sup>

> [1]中更多的引用：Parzen窗是概率密度函数估计的非参数方法。高斯函数实质上是一个基，每个以样本为均值的高斯函数构成了无数个基底。parzen窗本质是函数逼近的思想，用一组正交基无限逼近一个分布。另外parzen窗可选择另外的窗函数 而不仅是高斯函数。窗的宽度影响较大。

> [1]中更多的引用：具体的核函数可以用Gaussian窗函数也可以用别的。都差不多的。比较关键的是bandwidth selection问题。 



<span id="a1">[1]</span> [求助：高斯混合模型 VS Parzen窗方法用于密度估计](http://www.douban.com/group/topic/37885903/) 
