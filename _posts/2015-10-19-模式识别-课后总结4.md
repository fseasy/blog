---
layout: post
title: 模式识别-课后总结4
date: 2015-10-19
categories: 笔记
tags: 模式识别
onewords: 第四课主要讲非参数方法，包含parzen窗，k近邻方法。
---
> 主要讲非参数估计的技术，包含parzen窗，k近邻方法 。

> 编辑中

之前讲的贝叶斯决策理论、最大似然估计等都是在概率密度（分布函数）已知（知道先验知识，似然函数等信息 ，或者知道满足某种具体的分布）的情况下进行估计、预测。本章讲的内容则是在概率密度函数未知的情况下去做估计。

直观的想法是，我们先根据样本去估计一个概率密度分布，然后用该概率密度去进行预测。

最直观的用样本去估计概率密度的方法是直方图估计。就是把样本的分布按照某个区间划分为直方图表示，然后用这个表示概率密度分布。这个在数据比较稀疏、连续时往往不是很好表示，特别是针对高维数据，有点不是很好弄。（对了，决策树是不是就是可以这么来看呢？）

所以这章主要介绍了广泛适用的parzen窗和k近邻方法。

###parzen窗

parzen窗是用来做概率密度估计的。

在一个小区域$R$内，设体积为V，在总样本数为n的情况下有k个样本落在该小区域$R$内，我们可以认为落在该区域内的样本的概率为$P = \frac{k}{n}$(经验分布)，由于其体积为$V$，则概率密度为$p(x) = \frac{P}{V} = \frac{k}{V \cdot n}$。

> 关于落在$R$内的样本概率问题：的确比较直观的想法就是，用在该区域内的样本数除以总的样本数，用其经验分布来表示其概率。在树上则是通过二项分布来得出的。设落在该区域的概率为$P$，每个样本可以看做是在$P$下独立同分布的，则$k$的样本落在该区域可以认为满足二项分布，即 $P_k(P) = \begin{pmatrix} k \\ n \end{pmatrix}P^k(1-P)^{n-k}$ ， 该关于$P$的$P\_k$函数，在坐标轴上表现出来就是一个如正态分布的钟形曲线！其最大值在$P = \frac{k}{n}$下取得。所以我们估计落在该区域的概率为$P = \frac{k}{n}$

接下来就需要明确，小区域如何确定、体积如何表示、如何确定在区域内的点个数。

使用均匀窗（或者说是窗函数）会比较好解释这些问题。我们把区域设置为$d$维的超立方体，将待求的的点（d维空间下的向量）作为中心，边长为$h\_n$。设置$\sigma$函数，定义为

$$
\sigma(x) = \begin{cases}
1 , &\left | x_j \right | \le \frac{1}{2} , j=1 , 2, \cdots , d \\
0 , &其他情况
\end{cases}
$$

> 即每一维到原点的距离小于边长（即1）的一半。可以在图上

该函数表示为是以原点为中心，边长为1的超方体窗下，$d$维向量表示的点是否在该窗体内部。我们可以把其他非原点为中心、边长不为1的情况归一化到该原点窗的条件下。例如，对以$\vec x$为原点，$h\_n$为边长的超立方体，点$\vec x^i$通过如下转换

$$
x' = \frac{x - x^i}{h_n}
$$

可以变为在原点、长度为1的情况，就可以使用$\sigma$函数来判断该点是否在以$\vec x$为原点、长度为$h\_n$窗体内部了。

有了上面的准备，对于$n$个$d$维向量表示的样本集$\\{x^1 , x^2 , \cdots , x^n\\}$ ， 把它们作为一个分布的部分已知信息，我们就可以据此来估计该分布下任意一个位置的概率密度函数。

由最开始推算的公式，我们选择边长为$h\_n$的超方体，其体积为$V = h\_n^d$ ， 该超方体内部拥有的点数量$k$可以通过$\sigma$算出，即$k = \sum\_{i=1}^n{\sigma \left( \frac{x^i - x}{h\_n}\right)}$ ， 由此落在在超方体内点的概率为$P = \frac{k}{n}$，由$p(x ) = \frac{P}{V}$，带入上面的各表示式，有：

$$
p(x) = \frac{1}{ n h_n^d}\sum_{i=1}^{n}{\sigma \left ( \frac{x^i - x}{h_n}\right )}
$$

其中$h\_n$的选择往往很关键。通常用$h\_n = h\_0 * \frac{1}{\sqrt{n}}$来确定。但是$h\_0$又等于多少呢？

上述表示的概率密度函数，当然应该还要满足概率密度的意义。至少包含两条：

1. 任何位置大于等于0
    
    这个考察p(x)，很直观看出是满足的，不可能存在负数

2. 从负无穷积分到正无穷，和为1

    $$\begin{split}
    \int_{-\infty}^{\infty} \frac{1}{ n h_n^d}\sum_{i=1}^{n}{\sigma \left ( \frac{x^i - x}{h_n}\right )} dx &= \frac{1}{n} \sum_{i=1}^{n} \int_{-\infty}^{\infty}{\frac{1}{h_n^d} \sigma(\frac{x^i -x}{h_n})} dx \\
    &= \int_{-\infty}^{\infty}\sigma(u) du \\
    &= \int_{-\frac{1}{2}}^{\frac{1}{2}} 1 du
    &= 1
    \end{split}$$

    > 其中由$\int\_{-\infty}^{\infty}{\frac{1}{h\_n^d} \sigma(\frac{x^i -x}{h\_n})} dx$变为$\int\_{-\infty}^{\infty}\sigma(u) du$应该是用了变量替换，但是怎么就把$h\_n^d$消去了不是很确定。


除了上面的窗函数$\sigma$，还可以选择其他的核函数——就是使用核函数替换掉窗函数$\sigma$。

比如通常会使用高斯函数（标准正态分布函数）$k(x) = \frac{1}{\sqrt{2 \pi}} \exp(- \frac{x^2}{2})$来作为核函数。

**注意注意！**parzen窗方法也叫作`Kernel Density Estimation` ， 不同领域叫法不同。<sup><a href="#a1">[1]</a></sup>

> [1]中更多的引用：Parzen窗是概率密度函数估计的非参数方法。高斯函数实质上是一个基，每个以样本为均值的高斯函数构成了无数个基底。parzen窗本质是函数逼近的思想，用一组正交基无限逼近一个分布。另外parzen窗可选择另外的窗函数 而不仅是高斯函数。窗的宽度影响较大。

> [1]中更多的引用：具体的核函数可以用Gaussian窗函数也可以用别的。都差不多的。比较关键的是bandwidth selection问题。 





<span id="a1">[1]</span> [求助：高斯混合模型 VS Parzen窗方法用于密度估计](http://www.douban.com/group/topic/37885903/) 
