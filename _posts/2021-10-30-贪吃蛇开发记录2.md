---
layout: post
title: 贪吃蛇开发记录2
date: 2021-10-30
categories: 技术
tags: 贪吃蛇 Python QLearning
---

> 打圈问题优化；

前文提到，针对迭代收敛差的问题，主要考虑

1. 对 `snake状态表示` 增加方向状态
2. 减少动作循环（转圈）时 reward 的惩罚

目前上述两个方法都已经尝试。

1. 对`snake状态表示`，目前定义如下：

    | 特征类型         | 特征个数            | 每个特征取值            | 总特征空间 |
    |-----------------|-------------------|-----------------------|-----------|
    | 头距离四周障碍距离 | 4 （上、下、左、右） | 3个 （0、1、2+）        | $3^4$    |
    | 头距离food的距离  | 2 （横、纵）        | 3个 （0，正距离，负距离） | $3^2$    |
    | 头方向           | 1                 | 4（上、下、左、右）       | 4        |

    相比之前，变更如下：

    1. 新增`头方向`，为了稳妥，直接取了4个方向；
    2. 对距离取值做了裁剪，状态数变少（可能有负面影响，未进一步分析）

    同时也把这块代码重写了——之前写得太糙了。

2. 对转圈惩罚，简单地将其损失设置为

    ```python
    _step_after_last_gain += 1
    _ref_len = game_state.state_width + game_state.state_height
    times = _step_after_last_gain // _ref_len
    reward = - 1 / 1000 * min(times, 100000)
    ```

    之前是 `-1/1000 * 10^times`, reward 直接 overflow 了（QTable用的是numpy）……

上述改动后，比之前似乎是好了一点——看起来分数能到20+了；但是，转圈问题依然得到解决。同时上述reward的惩罚，还是不够合理：惩罚依然是随机的。

## 打圈问题优化

为什么会打圈。一种想法是，状态空间有点小，会不会不同的状态，在状态表征上一样，导致了行为冲突？这有一定的可能性，至少增大了打圈的概率吧。

但考虑打圈问题本质，还是snake的状态表征刻画不了打圈——也就是，虽然它在打圈，但是snake自己不知道。
这种咋整——在NN里面，那肯定得上序列建模啊——或者用规则特殊处理。在 QLearning 下，序列建模应该是不行，不太可能把长串的历史状态也给记录下来。
那我们就只能加手工特征： 是否在打圈！ 这样snake就能显式地直到自己在打圈了。难点就是怎么做这个特征了。

最后，reward针对打圈问题的惩罚是随机的，这可能破坏已经学好的一些处理逻辑。不过，如果我们的状态包含了“是否在打圈”，可能这个破坏影响也还好。

综上简单分析，当务之急还是先把“是否在打圈”给加上。然后再看看效果。

### “是否在打圈”特征

简单地，我们记录下在吃到食物前，snake头走过的位置集合。如果新位置在已走过的位置，那就直接认为它在打圈。也就是说，重复踩之前的位置，就认为在打圈。
这个判定实现起来简单，但没有直接判定”是否在打圈“。试试吧，说不定就足够了——一般情况下，头不需要回到以前走过的位置。

=> 结果出来了，从训练过程来看，打圈的概率大大降低了！ 果然有用呢！

现在训练出来，可以跑30+分了！