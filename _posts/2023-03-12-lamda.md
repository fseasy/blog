---
layout: post
title: LaMDA Notes | Paper Reading
date: 2023-03-12
categories: 技术 
tags: 对话系统 LLM
---
> ChatGPT 出来引爆了讨论。好久没看论文的我被迫调研了下 LaMDA.

LaMDA, 即 Language Models for Dialog Applications, 面向对话应用的语言模型. 来自 Google (猜测是 Google Research 下的 Brain team).

早在 2021 年 5 月，[谷歌博客][1]就发布名为 *LaMDA: our breakthrough conversation technology* 的文章，宣布了此技术。而直到 2022 年 1 月， Google Research 下面的[博客][2]才发布了细节并附上 [Arxiv 论文][3]链接, 其最后更新时间是 22 年 2 月，也即本次阅读的文章。

## 论文结构及作者

文章一共47页，正文 18 页，reference 6 页；正文除常规部分外，主要介绍了 LaMDA 预训练、指标定义、微调数据集构建及微调方法；结果上展示了指标层面的效果和在 domain grounding 上的示例结果；最后还有一个能源与碳足迹的估计。

附录 23 页，包含  Safety objectives and data collection， Crowdworker instructions for quality and groundedness， Selected example dialogs， Model hyper-parameters， Pre-training data composition， Pre-training and ﬁne-tuning results 等内容。

作者也有半页纸，第一排如下：

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha.

一作 Romal Thoppilan, 来自谷歌大脑(Senior Software Engineer, Google Research, Brain Team)，目前似乎已经去了 Character.AI. 

Daniel De Freitas, 完成这个工作后就早早地离开谷歌，创建了 Character.AI.

Jamie Hall, 没有太多资料，只看到也是 [Meena][4] 的作者（LaMDA 看起来延续了 Meena 的工作）。

Noam Shazeer, 同样是 Character.AI 的创建者；也是 Transformer(Attention is all you need)的核心研发。从[报道][5]看到, Noam 提出了缩放点积注意力、多头注意力和无参数位置表示，并成为几乎每一个细节的参与者。

Apoorv Kulshreshtha， 也是 Meena 的作者。



[1]: https://blog.google/technology/ai/lamda/ "LaMDA: our breakthrough conversation technology"
[2]: https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html "LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything"
[3]: https://arxiv.org/abs/2201.08239 "LaMDA: Language Models for Dialog Applications"
[4]: https://arxiv.org/abs/2001.09977 "Towards a Human-like Open-Domain Chatbot"
[5]: https://www.sohu.com/a/545930227_129720 "Transformer 五年，六位作者创立五家公司，仅一位作者还留在谷歌 "