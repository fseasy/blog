---
layout: post
title: 模式识别-课后总结2
date: 2015-09-18
categories: 笔记
tags: 模式识别
onewords: 贝叶斯决策论
---
> 第二课主要讲贝叶斯决策理论

### 定义

贝叶斯决策论是**解决模式分类问题**的一种基本**统计**途径。出发点是利用**概率的不同分类决策**与相应的**决策代价**之间的定量折中。它做了如下假设：

1. 决策问题可以使用概率的形式来描述

2. 所有有关的概率结构已知。

它**只是基于常识的判决过程的一种形式化**而已。

### 一个例子及一些概念

分类鲑鱼与鲈鱼 ： 

用术语表述，当每条鱼出现时其类别处于两种可能的状态：鲈鱼或鲑鱼。如果用$\omega$表示类别状态，当$\omega=\omega_1$时是鲈鱼，$\omega = \omega_2$时是鲑鱼。由于类别的状态不确定，故**我们可以假设$\omega$是由一个概率来描述其特性的随机变量**。

在实际观察之前，我们必须立即对下次将出现的鱼做出判断。我们将在*未看到具体鱼之前*的各类别鱼出现的频率信息称为*先验知识*。根据先验知识获得的预测概率称为**先验概率**。

> 先验概率，就是直接对下一个未知数据的所在类别的概率估计。 == 各类别出现的概率。

频率信息之外，还可以获得一些特征。这些特征量在不同的类别下有不同的概率分布。如“鱼的光泽”在鲑鱼和鲈鱼两个类别下分布不同。鲈鱼中光泽较高的比例高于鲑鱼。我们将某个特征表示为连续随机变量$x$ ， 该变量在具体类别$\omega$下的分布被称为在类别$\omega$下**类条件概率密度(class-conditional probability density)**，将其记为$P(x \| \omega)$。
**类条件概率密度函数也被称为似然函数(Likelihood)**

> 似乎可以这么认为： 运用主题模型的思想， 不同类别对应于不同的主题。而类条件概率密度，或者说似然，就是该特征（或者实例——实例就是多个特征的组合）在这个类别下出现（生成）的概率。这样就与后验概率区分开了。后验概率可以看做已经看到了实例，猜测（或者说计算）其在某类别（主题）下的概率。

我们看到了一个实例（鱼），求其属于某个类别（鲑鱼或是鲈鱼）的概率 $P(\omega \| x )$ , 这叫做**后验概率（因为我们先看到了实例）**。

后验概率与先验概率、类条件概率密度函数（似然函数）存在如下关系：

$$ p(\omega | x ) = \frac{ p(x | \omega) \cdot p(\omega)}{p(x)} $$

其中$p(x \| \omega)$是前面介绍的似然函数（这里就不再称呼为条件概率密度函数

$p(\omega)$ 是先验概率

p(x)称为证据因子(evidence)。这是一个不好解释的符号。我们可以认为它是该实例出现的概率（这是直观的理解，假设变量X符合一个分布，概率密度函数为f , 那么X = x 时，其概率密度（或者说在X=x的一个小区间内，其概率）为p(x) . *括号内的解释是自己的理解，没有科学依据*) 。书上说，可以把这个值认为是一个标量因子，这主要是为了使出现x的条件下所有类别的后验概率密度之和为1，即$\sum_j{p(\omega_j \| x)} = 1$ 。除以这个标量，使得这个式子满足概率条件。

以上的公式也称作`贝叶斯公式`。

> 贝叶斯公式的原始形式，应该是$p(\omega \| x) p(x) = p(x \| \omega) p(\omega)$ ， 两个式子都是表示随机变量$x$和$\omega$共同出现的概率，即联合概率. 直观地理解联合概率：$\omega$与$x$共同出现，要么$x$先出现，然后$x$出现的条件下$\omega$出现，即$p(x)p(\omega \| x)$; 或者是$\omega$先出现，然后在$\omega$出现的条件下$x$出现，即$p(\omega)p(x \| \omega)$ 。这二者是等价的。

### 损失函数（风险函数）

####最小错误率分类器 条件下

如果我们按照贝叶斯后验概率，每次取使后验概率最大的类别作为输出类别，即$ \omega_o = \arg\max_{j}{p(\omega_j \| x)} $ , 得到的分类器称为最小错误率分类器。

最小错误率分类器下的损失函数为：

$$R(w_i | x) = p(w_i | x ) = 1 - \sum_{t \ne i }{p(w_t | x)}$$

为什么以后验概率作为输出依据就是最小错误率的分类器？

错误率就是不选择实际类别的概率。$p(error \| x) = 1 - p(w_{right} \| x)$可以很直观的推出，相比选择其他类别，如果我们按照贝叶斯后验概率选择分类器的输出（即选择概率最大的作为输出），其错误率必然是最小的。

####最小风险（损失）分类器 条件下

我们定义*可能的类别（实际类别）序列*为 $ \\{ w_1 , w_2 , \cdots , w_n \\}$ ， *预测类别（或者说分类动作）序列*为$ \\{ \alpha_1 , \alpha_2 , \cdots , \alpha_n \\} $ （其实符号都是一样的，仅仅是为了区分实际与预测这二者不同的情景）

定义*将实际类别$w_j$判断为$\alpha_i$的风险（损失）*为$\lambda_{ij} = \lambda(\alpha_i \| w_j)$

由此定义我们将实例$x$分类为$\alpha_i$的损失函数（风险函数） 

$$ R(\alpha_i | x) = \sum_{j=1}^{n}{\lambda(\alpha_i | w_j) p(w_j | x)}$$

相比最小错误率分类器，我们添加了将不同类别错误分类的代价。如果要最小化错分代价，我们需要使用最小损失分类器。

（其实这个可以认为是带权值的分类器，为不同分类设置不同权值。这在实际中是有意义的。）

- 0-1损失

    我们定义一种误分类损失：

    $$\lambda_{ij} = \begin{cases}
    0 , &i = j \\
    1 , &i \ne j  
    \end{cases}$$

    在这种损失情况下，我们的最小风险分类器的损失函数变为：

    $$ R(\alpha_i | x) = \sum_{j=1}^{j=n}{[i \ne j]p(w_j | x)} = 1 - \sum_{j \ne i}{p(w_j | x)} $$

    （PS ， `[ condition ]`是指示函数，如果为真则为1 ， 否则为0）

    在0-1损失下，最小风险分类器变为了最小错误率分类器，因而，我们可以认为**最小错误率分类器时最小风险分类器的特例**


###判别函数（Descriminant Functions）

<p>
分类器依靠什么来分类呢？上面我们说了最小错误率和最小风险下的分类损失，其中最小错误率是依据后验概率来进行分类的，即$w_0 = \arg \max_{w_i}{p(w_i | x)} $, 最小风险是相似的写作 $ w_o = \arg \max_{w_i}\sum_{j=1}^{j=n} \lambda(\alpha_i , w_j) p(w_j | x)$
</p>

我们把能够将实例分类的函数统称为判别函数。判别函数输入是实例，输出是类别，内部可能使用不同逻辑。

判别函数仅仅是要多实例做一个分类，其可能并没有具体的含义。**通常一个判别函数，是在一个包含实际含义的表示式基础上，为减轻计算量或保证计算可行性进行一些简化但等价地处理得到的。**

<p>
仍然以上述以后验概率作为分类依据的分类器为例，它的判别函数可以就是：
$$\begin{align*}
f(x) &= \arg \max_{w_i}{ p(w_i | x)} = \arg \max_{w_i}{\frac{p(x | w_i)p(w_i)}{p(x)}} \\
     &= \arg \max_{w_i}{\frac{p(w_i | x)p(w_i)}{\sum_{j=1}^{n}{p(x\|w_j)p(w_j)}}} 
\end{align*}$$
</p>

其中${\sum_{j=1}^{n}{p(x\|w_j)p(w_j)}}$对所有的类别都是相同的，这对判别而言没有意义，故判别函数可以忽略该计算，因而变为：

$f(x) = \arg \max_{w_i}{p(x \| w_i)p(w_i)}$

由于$p(x \| w_i)$与$p(w_i)$可能都非常小，二者相乘结果可能低于计算机能够表示的最小值，故我们可以给整个式子取$ln$运算，由于$ln$能够保证单调性，故对于判别而言，是等价的 ， 结果为：

$f(x) = \arg \max_{w_i}{(\ln p(x \| w_i) + \ln p(w_i))}$

上述的这些变换在判别函数定义中非常常用。我们最初始的表达式往往有确切的统计含义 ， 但直接将其作为判别函数可能会造成计算资源浪费，甚至使计算不可行。使用`去除共有常数`、`对数处理`等手段进行等价化简，对于判别函数而言是意义重大的。

#### 二元分类问题的判别函数

由以上知识，对于分类，我们需要计算各个类别的分数值，然后求argmax 。特别的，对于二元分类，我们还可以简化其判别函数。

假设第一个类别的分数为$s_1(x)$ ， 第二个类别的分数为$s_2(x)$ ， 则我们可以定义$s = s_1(x) - s_2(x)$ , 如果结果大于0，则等价于第一类分数大，输出第一类；反之同理。由此，从表面上，我们简化掉了argmax过程，但从实际出发，将两个打分函数相减，可以进行更细节的合并，计算复杂度一般比分别计算两个分数低。

实际上，以分类器为例：如果是二元分类，我们仅需定义一个超平面即可，而传统方法需要定义两个。这相当于减少了一半的计算量。






