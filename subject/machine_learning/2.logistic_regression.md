---
layout: post
title: Logistic Regression 逻辑回归
date: 2023-02-27
categories: 技术
tags: LR 机器学习基础
---

$\rm{sigmoid}$ 导数如下：

$$\sigma(x)^{'} = \sigma(x)(1 - \sigma(x))$$

交叉熵损失函数为：

参考文档： https://zhuanlan.zhihu.com/p/299612493

=========== others ===========

低资源摘要生成：

摘要生成主要2个挑战：

1. 如何找到文章的核心信息
2. 如何生成流畅通顺的话

借用预训练模型，可以让生成通顺的话变得容易；但是找到核心的信息，特别是符合特定要求的核心信息还是一个挑战。

要解决这个，一般都是标数据。但生成式摘要标注成本比较高，所以如何用少量标注就可以提升模型对关键信息的捕获，这是一个值得研究的点。

我们的思路比较直接：摘要数据比较难标，但是标题数据一般天然存在，也比较容易获取较大的量。能不能让标题数据来辅助摘要模型获取关键信息？基于这个信息我们再少量标注一些目标摘要数据，就能得到一个相对好的效果？这就是我们的 motivation.

我们先验证了一下这个 motivation, 就是找了一批同时有标题和摘要的数据（newsroom），我们看下
1. 标题作为 predict 摘要，和 gold 摘要的 ROUGE 值；基于与标题字面近似选出来的句子作为 predict 摘要，和 gold 摘要的 ROUGE 值. 统计证明标题确实是有信息量的，但因为标题本身短，ROUGE 不占优；找句子出来效果更高 【没有看具体的结果了，不想找文档了】
2. 我们又看了下找出来的句子是不是都是 lead3 的。结果发现的 1/4 完全一样，2/4 部分一样，1/4 完全不同，说明标题相比 lead3 能够是有额外增益的。

这其实可以看做是一个远监督任务，让标题数据监督摘要对关键信息的识别和提取；

然后就是咋做了。

直接的做法就是先在标题数据上预训练，再拿预训练后的数据在摘要数据上微调；效果TODO；
另外因为现在已经有大模型了，而且 大模型预训练 + 领域预训练 + 任务微调也是比较work的范式，所以我们也试了这个方法，具体就是在 BART 模型上，用标题数据训练一下（因为本身就是 seq2seq 的框架，所以这个训练我们就直接做teacher-forcing的训练）；再用摘要数据微调；效果TODO；

因为这个项目我们主要还是想发论文，而上面的方法没啥新意，所以我们得想想其中存在的问题，然后针对性的去想一些解决方案。

