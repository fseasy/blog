---
layout: post
title: GLM 广义线性模型
date: 2023-02-27
categories: 技术
tags: GLM 机器学习基础
---

GLM(Generalized Linear Model), 广义线性模型，是从普通线性模型(ordinary linear model)
引申出来的。它是一种概念，定义的是一类模型，线性回归、逻辑回归和泊松回归，都属于广义线性模型。

> 要说哪怕文章比较完整地介绍好了 GLM，还得是[维基百科][_glm]。得好好看，应该比大部分博客讲得好。此外，[指数分布族和广义线性回归](https://shangzhih.github.io/zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html) 也算简洁易懂的。其他大部分博客都是缝合怪，包括我这篇……

> 在这里，线性模型(linear model)和线性回归(linear regression)是同义、等价的。
因为根据维基百科 [Linear model][_lm], 在统计学的回归模型语境下，二者是 synonymous 的。
本文里，为了对应，当我们说到广义线性模型时，我们相应地使用（一般）线性模型这个词；
而在说到具体回归模型时，我们使用线性回归这个术语。

## 普通线性模型

普通线性模型的表达式为： 

$$
\newcommand{\bs}{\boldsymbol}
y = {\bs{\beta}}^T [\bs{x};1] + \epsilon
$$

其中 
- $y$ 是因变量 $Y$ 的一个取值，机器学习里其一般是 scalar 的（即非多元）
- $\bs{x}$ 是自变量 $X$ 的一个观测（取值）， $[\bs{x};1] = \{x_1, x_2, \cdots, x_n, 1\}$ 是齐次向量
- $\bs{\beta}^T = \{\beta_1, \beta_2, \cdots, \beta_n, \beta_0\}$ 表示对 $[\bs{x};1]$ 各维度的线性组合（含 bias $\beta_0$），是待求解参数
- $\epsilon$ 是噪声项，表示白噪声过程，服从 0 均值、同方差的正态分布。

> 一定要注意这里的噪声项！它必须要有，不然  
1\. 没法解释为何观测的 $y$ 和预测的值 $\hat{y}$ 有差异;  
2\. 没有此项，右边乘出来就是一个确定值，没法与左边的随机变量相等。

> 关注本文的大小写(如 $Y$/$y$ 等)符号：  
目前不是很规范，也不是很清楚。目前假定大写的字母表示整个分布，或者数据集全部构成的列表（或矩阵），
而小写的字母则表示单个观测/样本。

线性模型对各变量的要求和设置如下：

1. 因变量 $Y$ 属于正态分布！
  
   如常用线性回归来建模的“房价预测”问题，其因变量 $Y = \textit{某区域房价}$ 就属于正态分布。

   > 这里有一个疑问： *$Y$ 的分布*，是指 $Y$ 作为单个随机变量自身的分布（与 $X$ 无关），还是说是 $Y$ given x 的条件分布？ 感觉是说随机变量本身的分布更合适？

2. 自变量观测 $\bs{x}$ 和未知参数 $\bs{\beta}$ 具有非随机性：即 $\bs{x}$ 是确定的（毕竟数据已经确定了），
$\bs{\beta}$ 虽然未知，但也是确定的。
3. 研究对象是给定 $X$ 下 $Y$ 的均值（期望）$E[Y \mid X]$. 用[维基百科][_glm]里的话，即 *the expected value of $Y$ conditional on $X$*

   我们求解的是条件期望！

> 以前学线性回归，根本没有考虑过 $Y$ 应该满足的分布， 单纯认为 $Y$ 是连续值即可。也没有想到我们求解的是 $E[Y \mid X]$. 

由上，可知普通线性模型只能处理 $Y$ 属于正态分布的场景，
能不能扩展一下上述定义，让自变量的线性组合，可以表达更多分布类型的因变量呢？ 可以，这便引出 GLM 了。

## 广义线性模型

1972 年，John Nelder 和 Robert Wedderburn 在 *Journal of the Royal Statistical Society, Series A* 发表文章 *Generalized Linear Model*, 将不同分布的因变量 $Y$ 与自变量 $X$ 的线性组合通过 `link-function` 联系起来，
从而实现了线性模型的泛化。

GLM 由如下部分组成：

1. 因变量 $Y$, 分布属于指数族(exponential family). 指数族分布是 GLM 的基础。 

   为什么要求是指数族？因为这是假设！见附录摘录的内容。

2. 线性组合 $\eta = \bs{X}\bs{\beta}$, 其中 $\bs{\beta}$ 是参数，未知但确定； $\bs{X}$ 是自变量（样本），同样确定。$\eta$ 也被称为 linear predictor

   注意和线性模型相比，这里没有了误差项 $\epsilon$. 为什么可以这样？可以参考附录 yihui 的论述——这个可以看做 GLM 建模思想超越线性模型的点。

3. 研究对象 $E[Y \mid X] = \mu$, $\mu$ 是期望值

4. 链接函数(`link-function`) $g$，使得 $E[Y \mid X] = g^{-1}(\bs{X}\bs{beta})$, 也即 $\mu = g^{-1}(\eta)$. 
   
   链接函数连接了 $E[Y\mid X]$ 和 linear predictor，是 GLM 的关键。

   > 注意，链接函数是取反函数，再作用到线性组合上的。也就是说，链接函数是直接作用到 $\mu$ 上的。至于为啥要这样？大概就是 assumption 吧。

相比线性模型，GLM 主要引入 2 个概念，分别是指数族和链接函数，以下分别介绍。

### 指数族

[指数族-张振虎的博客][exp_family_zhang]里对指数族有细致的描述，这里摘取一些结论的东西：

1. 指数族可以写作如下的格式:

   $$
   p(y \mid \theta) = \exp{\phi(\theta)^{T}T(y) + S(y) - A(\theta)}
   $$

   其中 $S(y)$ 是基础度量，$T(y)$ 是 $y$ 的统计充分量，而 $A(\theta)$ 是概率的归一化因子（分母）拿到 $\exp$ 里的值。
   之所以 $\phi(\theta)^{T}$ 有转置，是因为 $\theta$, $y$ 都可能是向量，这里转置相乘，结果是标量。

   进一步为了表示方便，一般令 $\eta = \phi(\theta)^{T}$, 从而上式写作关于 $\eta$ 的函数为：

   $$
   p(y \mid \eta) = \exp{\eta^{T}T(y) + S(y) - A(\eta)}
   $$

   $T(y)$ 前面就是 $\eta$, 没有别的变换，称其为指数族的**标准形式**(canonical form), $\eta$ 也叫标准参数(canonical parameter)或自然参数(natual parameter).

2. 指数族标准形式有一个很好的性质： $E[T(y\mid\eta)]$ 的期望，就是 $A(\eta)$ 的一阶导数(方差则是二阶导数)。

   怎么证明的？泛泛地看，因为前面说过 $A(\eta)$ 本来就是概率归一化因子的变形，也即它本就含 $p(y\mid\eta)$ 的积分；


这里看下简单的伯努利分布如何化为标准指数族形式：

伯努利分布通常来描述一个随机事件 $Y$ 是否出现（0/1），仅含一个参数 $\theta \in [0, 1]$. 如 $Y$ = 抛硬币1次正面向, 满足 $\theta = 0.5$ 的伯努利分布。
其概率（质量函数）一般写作如下形式：

$$
p(y \mid \theta) = \theta^{y}(1-\theta)^{1-y}
$$

将其改写为指数族形式（加 $\exp$）：

$$
\begin{split}
p(y\mid\theta) &= \theta^{y}(1-\theta)^{1-y} \\
               &= \exp \lbrace \ln(\theta^{y}(1-\theta)^{1-y})\rbrace \\
               &= \exp \lbrace y\ln\theta + (1-y)\ln(1-\theta) \rbrace \\
               &= \exp \lbrace y\ln\theta - y\ln(1-\theta) + \ln(1-\theta) \rbrace \\
               &= \exp \lbrace \ln(\frac{\theta}{1 - \theta})y + \ln(1 - \theta) \rbrace
\end{split}
$$

将上式化为 **标准形式（canonical form）**, 令 

$$
   \eta = \ln(\frac{\theta}{1 - \theta})
$$

则 $\theta = \frac{1}{1 + \exp\lbrace -\eta \rbrace}$，带入有

$$
p(y\mid\eta) = \exp \lbrace \eta y + \ln(\frac{\exp \lbrace -\eta \rbrace}{1 + \exp \lbrace -\eta \rbrace}) \rbrace
$$

对比指数族标准形式的定义（形参），可知：

$$
\begin{split}
   T(y) &= y \\
   A(\eta) &= - \ln(\frac{\exp \lbrace -\eta \rbrace}{1 + \exp \lbrace -\eta \rbrace}) \\
            &= \ln(1 + \exp\lbrace \eta \rbrace) \\
   S(y) &= 0
\end{split}
$$

**伯努利分布的期望** 可以直接由 $y * \theta + (1-y) * (1-theta)$ 得到，为 $E[y;\theta] = \theta$.

> $;\theta$ 这里表示以 $\theta$ 为参数.

不过我们也可以有前面摘录的第二点，从标准指数族的期望计算方式来计算——也即算 $A(\eta)$ 对 $\eta$ 的一阶导数。

$$
\begin{split}
E[y;\eta] &= \frac{dA(\eta)}{d\eta} \\
      &= \frac{1}{1 + \exp\lbrace -\eta \rbrace}
\end{split}
$$

带入前面的定义 $\eta = \ln(\frac{\theta}{1 - \theta})$, 有 

$$
   E[y;\theta] = \frac{1}{1 + \exp\lbrace - \ln(\frac{\theta}{1 - \theta}) \rbrace} = \theta
$$

> 这个算法不知道具体对不对？ 结果上倒是凑上了。
 

### 链接函数

前面说道链接函数 $g$ 是链接 $Y$ 的期望 $\mu$ 和 link predictor $\bs{X}\bs{\beta}$ 的，而且链接方式也已经定义： $\mu = g^{-1}(\eta)$, 或者 $g(\mu) = \bs{X}\bs{\beta}$ 
问题来了，它该怎么求解的？

> 注意，前面的link predictor 定义为了 $\eta$, 和指数族里的 $\eta$ 冲突了（缝合怪没办法）。这里我们保留指数族的 $\eta$, 而 link predictor 直接用原始变量表示。

这个问题困扰了我很久，我看了一些博客，没能看明白。今天认真读了下 wikipedia 的 GLM， 总算看清楚了：

1. There are **many** commonly used link functions, and their choice is informed by several considerations. 
2. There is **always a well-defined** canonical link function which is derived from the exponential of the response's density function. 
3. However, in some cases it makes sense to try to match the domain of the link function to the range of the distribution function's mean, or use a non-canonical link function for algorithmic purposes, for example Bayesian probit regression.

首先明确的就是，链接函数不是求解出来的，是定义出来的。

其次，总是存在一个 canonical link function, 性质是很好的； 但有时也不定用这个标准链接函数，可以看情况。比如在[广义线性模型中逻辑回归相关的logit连接函数的概率密度怎么理解？](https://www.zhihu.com/question/282920402/answer/432082713) 就讨论了逻辑回归的两种链接函数(probit 和 logit).

我们这里关注 canonical link function, 维基百科上写到：

is the function that expresses $\theta$ in terms of $\mu$, ...the function that **maps the density function into its canonical form**.

本文参数和维基百科的参数定义不一致； 没理解错的话，标准链接函数就是前面那个自然参数的定义，即前面的 $\eta = \phi(\theta)$, 而 $\theta$ 可以用 $\mu$ 来表示。

比如，对符合伯努利分布的 $Y$ 而言， 其 

$$\eta = \ln\frac{\theta}{1 - \theta} = \ln\frac{\mu}{1 - \mu}$$ 

则有 $\ln\frac{\mu}{1 - \mu} = \bs{X}\bs{\beta}$. 
可以求解出 $\mu = \rm{sigmoid}(-\bs{X}\bs{\beta})$，这也是常见的逻辑回归概率公式的由来。


## 附录

### [GLM建模过程](https://shangzhih.github.io/zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html)：

总结一下GLM的建模过程:

1. 根据问题在指数分布族中选择一种分布作为对 $y$ 的假设
2. 计算该分布下的$\eta$，即把分布参数 $\theta$ 变为自然参数 $\eta$
3. 计算该分布的期望，将其用 $\eta$ 表示。例如伯努利分布时的 $\mu = \frac{1}{1 + \exp\lbrace -\eta \rbrace}$
4. 根据GLM的假设, 将上面的 $\eta$ 替换为 $\bs{X}\bs{\beta}$, 即得到GLM模型

### [统计之都文章评论摘选][cosx]

**yihui**: 并不是说把推导什么的都放上来，那样不如干脆推荐读者看书好了。把指数分布族的形式写出来的话，这件事情会明了许多，比如为什么Logistic回归经常用logit联接函数（我见过一些民科的吐血解释）、为什么那个散布参数是个“讨厌参数”（极大似然估计可以扔掉它不管），等等。更深层的意义在于，广义线性模型不是简单的推广分布族，它是另一种思想。普通的回归的中心是加性误差，而GLM则是把模型分成两个组成成分来考虑，一个系统成分（自变量线性组合），一个随机成分（因变量的概率分布），二者用连接函数连起来。你可以说GLM是普通回归的推广，但我觉得这样有点低估它在统计建模思想上的突破。一己之见，供参考，不一定对。

### [广义线性模型中逻辑回归相关的logit连接函数的概率密度怎么理解？-子元回答](https://www.zhihu.com/question/282920402/answer/432082713)

Logit 和 probit 都可以作为 link function 用在面向 binary data 的 GLM 上，其中 logit 是 canonical link function，probit 不是。

先回顾一下指数分布族。指数分布族是一种最大熵分布：在给定的 support 上，它满足约束 $E[\mathbf{T}(X)]=\bs{\mu}E[\mathbf{T}(X)]=\bs{\mu}$ 且微分熵 $E[\log(X)]$ 最大。
它有一般形式 $f(x|\bs{\theta})=h(x)\exp(\bs{\eta}(\bs{\theta})^\mathrm{T}\mathbf{T}(x)-A(\bs{\theta}))$ ，其中约束中所选的 $\mathbf{T}$ 是该分布的充分统计量。
当 $X\in\{0,1\}$ 且约束为 $E[X]=\mu$ 时，我们得到参数 $\theta=\mu$ 的 Bernoulli 分布，且此时 $\eta(\theta)=\ln\frac{\theta}{1-\theta}$，也就是 logit 函数。  

回到 GLM 。GLM 讲的主要是，当响应 $y$ 服从某个指数分布族时，应该如何做回归。在 GLM 中，形状跟 $\bs{\eta}(\bs{\theta})$ 一样的 link function 叫做 canonical link function（此时相当于对 $\bs{\eta}$ 做线性回归， $\bs{\eta}=\mathbf{X}\bs{\beta}+\bs{\epsilon}$），它有一些理论/计算/实验上的良好性质，比如说，充分统计量是 $\mathbf{X}^\mathrm{T}\mathbf{y}$ 、拟合 $\bs{\beta}$ 时牛顿法跟 Fisher scoring 一致、方便做 retrospective studies 等等（见 McCullagh 和 John Nelder 的 Generalized Linear Models）。

### [为什么广义线性模型（GLM）要求被解释变量属于指数分布族（Exponential Families）？-知乎-莘縣陽谷回答](https://www.zhihu.com/question/47637500/answer/530659678)

题主陷入了reasoning的死循环了。其实 exponential fammily 是 GLM 的一个 assumption. 在数学或者逻辑学中assumption是不需证明的。
换句话说，当相互独立的响应变量服从exponential family 时，才能够建立
response variable given canonical parammeter 这个条件期望与 linear predictor 的关系，这就是GLM.

### 其他参考

https://zhuanlan.zhihu.com/p/124757082 这个文章比较完善，看起来也参考了统计之都的文章。

https://zhuanlan.zhihu.com/p/49267988 伯努利和二项分布

[^1]: 参考[维基百科-Generalized linear model][_glm]: Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression.

[_glm]: https://en.wikipedia.org/wiki/Generalized_linear_model "Generalized linear model"

[_lm]: https://en.wikipedia.org/wiki/Linear_model "Linear model"

[exp_family_zhang]: https://zhangzhenhu.github.io/blog/probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html "3. 指数族"

[cosx]: https://cosx.org/2011/01/how-does-glm-generalize-lm-assumption/ "How-does-glm-generalize-lm-assumption"