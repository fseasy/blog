---
layout: post
title: GLM 广义线性模型
date: 2023-02-27
categories: 技术
tags: GLM 机器学习基础
---

GLM(Generalized Linear Model), 广义线性模型，是从普通线性模型(ordinary linear model)
引申出来的。它是一种概念，定义的是一类模型，线性回归、逻辑回归和泊松回归，都属于广义线性模型。

> 在这里，线性模型(linear model)和线性回归(linear regression)是同义、等价的。
因为根据维基百科 [Linear model][_lm], 在统计学的回归模型语境下，二者是 synonymous 的。
本文里，为了对应，当我们说到广义线性模型时，我们相应地使用（一般）线性模型这个词；
而在说到具体回归模型时，我们使用线性回归这个术语。

## 普通线性模型

普通线性模型的表达式为： 

$$
\newcommand{\bs}{\boldsymbol}
y = {\bs{\beta}}^T [\bs{x};1] + \epsilon
$$

其中 
- $y$ 是因变量 $Y$ 的一个取值，机器学习里其一般是 scalar 的（即非多元）
- $\bs{x}$ 是自变量 $X$ 的一个观测（取值）， $[\bs{x};1] = \{x_1, x_2, \cdots, x_n, 1\}$ 是齐次向量
- $\bs{\beta}^T = \{\beta_1, \beta_2, \cdots, \beta_n, \beta_0\}$ 表示对 $[\bs{x};1]$ 各维度的线性组合（含 bias $\beta_0$），是待求解参数
- $\epsilon$ 是噪声项，表示白噪声过程，服从 0 均值、同方差的正态分布。

> 一定要注意这里的噪声项！它必须要有，不然  
1\. 没法解释为何观测的 $y$ 和预测的值 $\hat{y}$ 有差异;  
2\. 没有此项，右边乘出来就是一个确定值，没法与左边的随机变量相等。

> 关注本文的大小写(如 $Y$/$y$ 等)符号：  
目前不是很规范，也不是很清楚。目前假定大写的字母表示整个分布，或者数据集全部构成的列表（或矩阵），
而小写的字母则表示单个观测/样本。

线性模型对各变量的要求和设置如下：

1. 因变量 $Y$ 属于正态分布！
  
   如常用线性回归来建模的“房价预测”问题，其因变量 $Y = \textit{某区域房价}$ 就属于正态分布。

   需要注意，这里 *$Y$ 的分布*，是指 $Y$ 作为单个随机变量自身的分布， 与 $X$ 无关。

2. 自变量观测 $\bs{x}$ 和未知参数 $\bs{\beta}$ 具有非随机性：即 $\bs{x}$ 是确定的（毕竟数据已经确定了），
$\bs{\beta}$ 虽然未知，但也是确定的。
3. 研究对象是给定 $X$ 下 $Y$ 的均值（期望）$E[Y \mid X]$. 用[维基百科][_glm]里的话，即 *the expected value of $Y$ conditional on $X$*

   再次说明，因为噪声 $\epsilon$ 的存在，在确定了 $\bs{x}$ 和 $\bs{\beta}$ 后，$y$ 依然是一个分布而非确定值！
   通过求解期望，就将 $\epsilon$ 去掉了，因为其期望为 0.

> 以前学线性回归，根本没有考虑过 $Y$ 应该满足的分布， 单纯认为 $Y$ 是连续值即可。也没有想到我们求解的是 $E[Y \mid X]$. 

由上，可知普通线性模型只能处理 $Y$ 属于正态分布的场景，
能不能扩展一下上述定义，让自变量的线性组合，可以表达更多分布类型的因变量呢？ 可以，这便引出 GLM 了。

## 广义线性模型

1972 年，John Nelder 和 Robert Wedderburn 在 *Journal of the Royal Statistical Society, Series A* 发表文章 *Generalized Linear Model*, 将不同分布的因变量 $Y$ 与自变量 $X$ 的线性组合通过 `link-function` 联系起来，
从而实现了线性模型的泛化。

GLM 有如下要求和设置：

1. $Y$ 服从某个分布，这个分布必须来自指数族分布(exponential family).
   指数族分布是一大类概率分布(family)，正态分布、二项分布、泊松分布和伽马分布等都在其中。
2. 存在线性组合 $\eta = \bs{X}\bs{\beta}$, 其中 $\bs{\beta}$ 未知但是确定的； $\bs{X}$ 同样确定

   这里似乎没有了误差项 $\epsilon$? 毕竟定义的是 $\eta$, 它可能就是被定义为确定值吧。

   $\eta$ 也被称为 linear predictor.

3. 研究对象同样是 $E[Y \mid X] = \mu$, $\mu$ 是期望值.

4. 额外定义一个链接函数(`link-function`) $g$，使得 $E[Y \mid X] = g^{-1}(\bs{X}\bs{beta})$, 也即 $\mu = g^{-1}(\eta)$.
   
   注意，链接函数是取反函数，再作用到线性组合上的。也就是说，链接函数是直接作用到 $\mu$ 上的。
   为啥要多此一举，用 $g^{-1}$ 而不是用一个非逆函数 $f$ 这种？或者把 $g$ 写到昨天的 $\mu$ 上？ 
   不知道为为啥，目前看到的所有的资料都是上面的写法，或许约定俗成吧。

由上，就是 GLM 的定义了。推广看起来挺简单，但这涉及到了一个新概念，**指数族分布**， 这也是 GLM 的核心。

指数族分布有很多形式，





## 附录

[GLM建模过程](https://shangzhih.github.io/zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html)：


总结一下GLM的建模过程。

根据问题在指数分布族中选择一种分布作为对y的假设
- 计算该分布下的η，实际上η=η(wT)，其中 wT 为该分布的真实参数，而η
只是以wT
为参数的一个link function
计算该分布的期望，将其用η
表示，例如上面伯努利分布时的y=ϕ=11+e−η
根据GLM的假设替换η=θTx
即得到GLM模型
将这些知识都串联起来，就能更好的理解不同回归模型下的前提假设及其link function的选择了

其中指数分布的期望： https://zhangzhenhu.github.io/blog/probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments

统计之都文章评论摘选：

> yihui: 并不是说把推导什么的都放上来，那样不如干脆推荐读者看书好了。把指数分布族的形式写出来的话，这件事情会明了许多，比如为什么Logistic回归经常用logit联接函数（我见过一些民科的吐血解释）、为什么那个散布参数是个“讨厌参数”（极大似然估计可以扔掉它不管），等等。更深层的意义在于，广义线性模型不是简单的推广分布族，它是另一种思想。普通的回归的中心是加性误差，而GLM则是把模型分成两个组成成分来考虑，一个系统成分（自变量线性组合），一个随机成分（因变量的概率分布），二者用连接函数连起来。你可以说GLM是普通回归的推广，但我觉得这样有点低估它在统计建模思想上的突破。一己之见，供参考，不一定对。

知乎-[子元的回答](https://www.zhihu.com/question/282920402/answer/432082713)：

> Logit 和 probit 都可以作为 link function 用在面向 binary data 的 GLM 上，其中 logit 是 canonical link function，probit 不是。先回顾一下指数分布族。指数分布族是一种最大熵分布：在给定的 support 上，它满足约束 E[T(X)]=μE[\mathbf{T}(X)]=\bm{\mu}E[\mathbf{T}(X)]=\bm{\mu} 且微分熵 E[log⁡(X)]E[\log(X)]E[\log(X)] 最大。它有一般形式 f(x|θ)=h(x)exp⁡(η(θ)TT(x)−A(θ))f(x|\bm{\theta})=h(x)\exp(\bm{\eta}(\bm{\theta})^\mathrm{T}\mathbf{T}(x)-A(\bm{\theta}))f(x|\bm{\theta})=h(x)\exp(\bm{\eta}(\bm{\theta})^\mathrm{T}\mathbf{T}(x)-A(\bm{\theta})) ，其中约束中所选的 T\mathbf{T}\mathbf{T} 是该分布的充分统计量。当 X∈{0,1}X\in\{0,1\}X\in\{0,1\} 且约束为 E[X]=μE[X]=\muE[X]=\mu 时，我们得到参数 θ=μ\theta=\mu\theta=\mu 的 Bernoulli 分布，且此时 η(θ)=ln⁡θ1−θ\eta(\theta)=\ln\frac{\theta}{1-\theta}\eta(\theta)=\ln\frac{\theta}{1-\theta} ，也就是 logit 函数。  
回到 GLM 。GLM 讲的主要是，当响应 y 服从某个指数分布族时，应该如何做回归。在 GLM 中，形状跟 $\bm{\eta}(\bm{\theta})\bm{\eta}(\bm{\theta})$ 一样的 link function 叫做 canonical link function（此时相当于对 η\bm{\eta}\bm{\eta} 做线性回归， $\bm{\eta}=\mathbf{X}\bm{\beta}+\bm{\epsilon}\bm{\eta}=\mathbf{X}\bm{\beta}+\bm{\epsilon} ）$，它有一些理论/计算/实验上的良好性质，比如说，充分统计量是 XTy\mathbf{X}^\mathrm{T}\mathbf{y}\mathbf{X}^\mathrm{T}\mathbf{y} 、拟合 β\bm{\beta}\bm{\beta} 时牛顿法跟 Fisher scoring 一致、方便做 retrospective studies 等等（见 McCullagh 和 John Nelder 的 Generalized Linear Models）。

知乎-莘縣陽谷：[为什么广义线性模型（GLM）要求被解释变量属于指数分布族（Exponential Families）？](https://www.zhihu.com/question/47637500/answer/530659678)

题主陷入了reasoning的死循环了。其实 exponential fammily 是 GLM 的一个 assumption. 在数学或者逻辑学中assumption是不需证明的。
换句话说，当相互独立的响应变量服从exponential family 时，才能够建立
response variable given canonical parammeter 这个条件期望与 linear predictor 的关系，这就是GLM.



参考： https://www.jianshu.com/p/9c61629a1e7d
https://zhuanlan.zhihu.com/p/64157078

https://zhuanlan.zhihu.com/p/420499972 更详细

https://cosx.org/2011/01/how-does-glm-generalize-lm-assumption/ 这个统计之都的，评论里有yihui的，很有启发！

https://zhuanlan.zhihu.com/p/124757082 这个文章比较完善，看起来也参考了统计之都的文章。

伯努利和二项分布： https://zhuanlan.zhihu.com/p/49267988

[^1]: 参考[维基百科-Generalized linear model][_glm]: Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression.

[_glm]: https://en.wikipedia.org/wiki/Generalized_linear_model "Generalized linear model"

[_lm]: https://en.wikipedia.org/wiki/Linear_model "Linear model"