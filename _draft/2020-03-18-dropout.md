---
layout: post
title: Dropout
date: 2020-03-18
categories: 技术 
tags: dropout
onewords: dropout具体是如何做的，有哪些要注意？
---

> Dropout, 早已是NN中不可或缺的一个组件。在BN出来的时候，dropout似乎受到了挑战。但现在看来，它依然存在于广泛的任务中。

## 相关论文

[1] Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012. 

> 引用数 4764， 首次使用了dropout技术

[2] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.

> 引用 59K， 大名鼎鼎的AlexNet(一作的名字)，引爆神经网络的应用！使用了dropout技术；

[3] Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural networks from overfitting[J]. The journal of machine learning research, 2014, 15(1): 1929-1958.

> 引用 18K; JMLR

[4] Bouthillier X, Konda K, Vincent P, et al. Dropout as data augmentation[J]. arXiv preprint arXiv:1506.08700, 2015.

> 46引用；


## 背景 / 要解决的问题




## 解决思路


## 具体做法


> 生成dropout矩阵，一般用的是binomial分布； numpy中的binomial接口如下：
> numpy.random.binomial(n, p, size), 其中n表示每个输出，要抛多少次硬币——决定了每个元素的值上限； p是成功的概率，
> 也就是抛一次为1的概率； size是shape

## 效果


